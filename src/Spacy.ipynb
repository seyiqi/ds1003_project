{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import sys,os,os.path\n",
    "python_path = ['', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python27.zip', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-darwin', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac/lib-scriptpackages', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-tk', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-old', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg', '/Users/Melancardie/Dropbox/Documents/My School/NYU/Spring 2017/DS-GA 1008/HW/hw3/ALI']\n",
    "\n",
    "for p in python_path:\n",
    "    if p not in sys.path:\n",
    "        sys.path.ap\n",
    "        pend(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "import time\n",
    "import nltk\n",
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I like green eggs and ham.A woman is walking through the door."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Spacy load ###\n",
    "# python -m spacy.en.download all\n",
    "begin=time.time()\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "begin=time.time()\n",
    "doc = nlp(u'I like green eggs and ham.A woman is walking through the door.')\n",
    "doc\n",
    "#for np in doc.noun_chunks:\n",
    "#    print(np.text, np.root.text, np.root.dep_, np.root.head.text)\n",
    "    # I I nsubj like\n",
    "    # green eggs eggs dobj like\n",
    "    # ham ham conj eggs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00140595436096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[I,\n",
       " like,\n",
       " green,\n",
       " eggs,\n",
       " and,\n",
       " ham,\n",
       " .,\n",
       " A,\n",
       " woman,\n",
       " is,\n",
       " walking,\n",
       " through,\n",
       " the,\n",
       " door,\n",
       " .]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### tokenization ###\n",
    "begin=time.time()\n",
    "doc = nlp(u'I like green eggs and ham.A woman is walking through the door.')\n",
    "tokenlist=[]\n",
    "for token in doc:\n",
    "    tokenlist.append(token)\n",
    "print(time.time()-begin)\n",
    "tokenlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "I like green eggs and ham.\n",
      "A woman is walking through the door.\n"
     ]
    }
   ],
   "source": [
    "# Get first token of the processed document\n",
    "token = doc[0]\n",
    "print(token)\n",
    "\n",
    "# Print sentences (one sentence per line)\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I - PRON\n",
      "like - VERB\n",
      "green - ADJ\n",
      "eggs - NOUN\n",
      "and - CCONJ\n",
      "ham - NOUN\n",
      ". - PUNCT\n",
      "A - DET\n",
      "woman - NOUN\n",
      "is - VERB\n",
      "walking - VERB\n",
      "through - ADP\n",
      "the - DET\n",
      "door - NOUN\n",
      ". - PUNCT\n"
     ]
    }
   ],
   "source": [
    "# For each token, print corresponding part of speech tag\n",
    "for token in doc:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0521030426025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'like',\n",
       " 'green',\n",
       " 'eggs',\n",
       " 'and',\n",
       " 'ham.A',\n",
       " 'woman',\n",
       " 'is',\n",
       " 'walking',\n",
       " 'through',\n",
       " 'the',\n",
       " 'door',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin=time.time()\n",
    "token=nltk.word_tokenize('I like green eggs and ham.A woman is walking through the door.')\n",
    "print(time.time()-begin)\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longest Common Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Code to find the number of tokens longest common sequence ### \n",
    "def lcs(xstr, ystr):\n",
    "    if not xstr or not ystr:\n",
    "        return 0\n",
    "    x, xs, y, ys = xstr[0], xstr[1:], ystr[0], ystr[1:]\n",
    "    if x == y:\n",
    "        return 1 + lcs(xs, ys)\n",
    "    else:\n",
    "        return max(lcs(xstr, ys), lcs(xs, ystr))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0028388500213623047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin=time.time()\n",
    "doc = nlp('I like green eggs and ham.A woman is walking through the door.')\n",
    "lcs_num=lcs(doc,doc)\n",
    "print(time.time()-begin)\n",
    "\n",
    "lcs_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002022981643676758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin=time.time()\n",
    "token=nltk.word_tokenize('I like green eggs and ham. A woman is walking through the door.')\n",
    "lcs_num=lcs(token,token)\n",
    "print(time.time()-begin)\n",
    "lcs_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### edit_distance ###\n",
    "\n",
    "token2=nltk.word_tokenize('I dont like red eggs and ham. A man is walking through the door.')\n",
    "nltk.edit_distance(token,token2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{shot: [I, elephant], elephant: [an, in], in: [pajamas], pajamas: [my]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### dependency parsing ###\n",
    "#Write a function that walks up the syntactic tree of the given token and collects all tokens to the root token (including root token).\n",
    "doc=nlp(u\"I shot an elephant in my pajamas\")\n",
    "\n",
    "def tokens_to_root(token):\n",
    "    \"\"\"\n",
    "    Walk up the syntactic tree, collecting tokens to the root of the given `token`.\n",
    "    :param token: Spacy token\n",
    "    :return: list of Spacy tokens\n",
    "    \"\"\"\n",
    "    tokens_to_r = []\n",
    "    while token.head is not token:\n",
    "        tokens_to_r.append(token)\n",
    "        token = token.head\n",
    "        tokens_to_r.append(token)\n",
    "\n",
    "    return tokens_to_r\n",
    "\n",
    "# For every token in document, print it's tokens to the root\n",
    "#for token in doc:\n",
    "#    print('{} --> {}'.format(token, tokens_to_root(token)))\n",
    "\n",
    "# Print dependency labels of the tokens\n",
    "#for token in doc:\n",
    "#    print('-> '.join(['{}-{}'.format(dependent_token, dependent_token.dep_) for dependent_token in tokens_to_root(token)]))\n",
    "\n",
    "child_dict = {}\n",
    "for i in doc:\n",
    "    if i.head != i:\n",
    "        if i.head not in child_dict:\n",
    "            child_dict[i.head]=[]\n",
    "        child_dict[i.head].append(i) \n",
    "    #print(\"{0} -> {1}\".format(i,i.head))\n",
    "\n",
    "\n",
    "child_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Recognition\n",
    "#### The entity recognition system only recognizes entity if it starts with capital letter\n",
    "#### It failed to recognize the entity if it is the first word in a sentence\n",
    "#### It also recognizes date and ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'PERSON'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print all named entities with named entity types\n",
    "doc_1 = nlp(u'WHO IS JAY?')\n",
    "doc_2 = nlp(u'Who is JAY?')\n",
    "doc_3=nlp(u'Hpp is Happy')\n",
    "doc_4=nlp(u'Xiaoming birthday is May 3rd')\n",
    "doc_5=nlp(u'The Xiaomings birthday is May 3rd')\n",
    "doc_6=nlp(u'The Xiaomings birthday is may 3rd')\n",
    "doc_2.ents[0]\n",
    "doc_1.ents[0].label_\n",
    "\n",
    "# for ent in doc_2.ents:\n",
    "#     print('DOC 2')\n",
    "#     print('{} - {}'.format(ent, ent.label_))\n",
    "# for ent in doc_1.ents:\n",
    "#     print('DOC 1')\n",
    "#     print('{} - {}'.format(ent, ent.label_))\n",
    "# for ent in doc_3.ents:\n",
    "#     print('DOC 3')\n",
    "#     print('{} - {}'.format(ent, ent.label_))\n",
    "# for ent in doc_4.ents:\n",
    "#     print('DOC 4')\n",
    "#     print('{} - {}'.format(ent, ent.label_))    \n",
    "# for ent in doc_5.ents:\n",
    "#     print('DOC 5')\n",
    "#     print('{} - {}'.format(ent, ent.label_))\n",
    "# for ent in doc_6.ents:\n",
    "#     print('DOC 6')\n",
    "#     print('{} - {}'.format(ent, ent.label_)) \n",
    "#len(doc_1.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun Chunks Recogniztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity\n",
      "U.S. - GPE\n",
      "US - GPE\n",
      "Noun Chunk:\n",
      "[An F1 visa, a nonimmigrant visa, the U.S., You, an F1 visa application, you, the US, a university or college, high school, private elementary school, seminary, conservatory, language training program, other academic institution]\n"
     ]
    }
   ],
   "source": [
    "### Using F1 Visa Wiki sentence as an example to compare entity recognition vs noun chunk\n",
    "doc_7=nlp(u'An F1 visa is a nonimmigrant visa for those wishing to study in the U.S. You must file an F1 visa application if you plan on entering the US to attend a university or college, high school, private elementary school, seminary, conservatory, language training program, or other academic institution.')\n",
    "print('Entity')\n",
    "for ent in doc_7.ents:\n",
    "    print('{} - {}'.format(ent, ent.label_))\n",
    "print('Noun Chunk:')\n",
    "print([chunk for chunk in doc_7.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Noun chunks helps to put F1- visa together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token probability - similar/related feature as TF-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(An, ',', -9.560046195983887)\n",
      "(F1, ',', -19.579313278198242)\n",
      "(visa, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(nonimmigrant, ',', -19.579313278198242)\n",
      "(visa, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(those, ',', -7.039104461669922)\n",
      "(wishing, ',', -19.579313278198242)\n",
      "(to, ',', -3.83851957321167)\n",
      "(study, ',', -9.603719711303711)\n",
      "(in, ',', -4.585874080657959)\n",
      "(the, ',', -3.425445795059204)\n",
      "(U.S., ',', -9.18703842163086)\n",
      "(You, ',', -6.308290958404541)\n",
      "(must, ',', -8.042095184326172)\n",
      "(file, ',', -9.593122482299805)\n",
      "(an, ',', -5.953293800354004)\n",
      "(F1, ',', -19.579313278198242)\n",
      "(visa, ',', -19.579313278198242)\n",
      "(application, ',', -10.08004379272461)\n",
      "(if, ',', -5.885849475860596)\n",
      "(you, ',', -4.547972679138184)\n",
      "(plan, ',', -8.751087188720703)\n",
      "(on, ',', -5.263686656951904)\n",
      "(entering, ',', -11.492071151733398)\n",
      "(the, ',', -3.425445795059204)\n",
      "(US, ',', -7.79523229598999)\n",
      "(to, ',', -3.83851957321167)\n",
      "(attend, ',', -11.281272888183594)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(university, ',', -10.434051513671875)\n",
      "(or, ',', -5.715355396270752)\n",
      "(college, ',', -8.981049537658691)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(high, ',', -8.075313568115234)\n",
      "(school, ',', -8.195212364196777)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(private, ',', -9.050930976867676)\n",
      "(elementary, ',', -19.579313278198242)\n",
      "(school, ',', -8.195212364196777)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(seminary, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(conservatory, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(language, ',', -8.811477661132812)\n",
      "(training, ',', -10.32292366027832)\n",
      "(program, ',', -9.13428020477295)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(or, ',', -5.715355396270752)\n",
      "(other, ',', -6.661053657531738)\n",
      "(academic, ',', -10.989052772521973)\n",
      "(institution, ',', -11.091423034667969)\n",
      "(., ',', -3.0729479789733887)\n"
     ]
    }
   ],
   "source": [
    "# For every token in doc_2, print log-probability of the word, estimated from counts from a large corpus \n",
    "for token in doc_7:\n",
    "    print(token, ',', token.prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "()\n",
      "0.569403057566\n",
      "0.323890771731\n",
      "0.33899602553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Apples"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### similarity ###\n",
    "# For a given document, calculate similarity between 'apples' and 'oranges' and 'boots' and 'hippos'\n",
    "doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "doc2 = nlp(u\"I like pears but not oranges.\")\n",
    "apples = doc[0]\n",
    "oranges = doc[2]\n",
    "boots = doc[6]\n",
    "hippos = doc[8]\n",
    "print(apples.similarity(oranges))\n",
    "print(boots.similarity(hippos))\n",
    "\n",
    "print()\n",
    "# Print similarity between sentence and word 'fruit'\n",
    "apples_sent, boots_sent = doc.sents\n",
    "fruit = doc.vocab[u'fruit']\n",
    "print(apples_sent.similarity(fruit))\n",
    "print(boots_sent.similarity(fruit))\n",
    "print(doc.similarity(doc2))\n",
    "apples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of nltk dependency grammar \n",
    "#### not useful for dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'shot' -> 'I' | 'elephant'\n",
      " 'in' -> 'pajamas'\n",
      " 'elephant' -> 'an' | 'in'\n",
      " 'pajamas' -> 'my'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{shot: [I, elephant], elephant: [an, in], in: [pajamas], pajamas: [my]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_list(word_list):\n",
    "    str_list = [\"\\'{0}\\'\".format(i) for i in word_list]\n",
    "    return \" | \".join(str_list)\n",
    "for i in child_dict:\n",
    "    print(\" \\'{0}\\' -> {1}\".format(str(i), print_list(child_dict[i])))\n",
    "#print_list(child_dict.values()[1])\n",
    "#child_dict\n",
    "child_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency grammar with 6 productions\n",
      "  'shot' -> 'I'\n",
      "  'shot' -> 'elephant'\n",
      "  'elephant' -> 'an'\n",
      "  'elephant' -> 'in'\n",
      "  'in' -> 'pajamas'\n",
      "  'pajamas' -> 'my'\n"
     ]
    }
   ],
   "source": [
    "groucho_dep_grammar = nltk.DependencyGrammar.fromstring(\"\"\"\n",
    "'shot' -> 'I' | 'elephant'\n",
    "'elephant' -> 'an' | 'in'\n",
    "'in' -> 'pajamas'\n",
    "'pajamas' -> 'my'\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "print(groucho_dep_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(shot I (elephant an (in (pajamas my))))\n"
     ]
    }
   ],
   "source": [
    "pdp = nltk.ProjectiveDependencyParser(groucho_dep_grammar)\n",
    "sent = 'I shot an elephant in my pajamas'.split()\n",
    "trees = pdp.parse(sent)\n",
    "for tree in trees:\n",
    "     print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'bright', u'red']\n",
      "[u'on']\n",
      "(u'Credit and mortgage account holders', u'NOUN', u'nsubj', u'submit')\n",
      "(u'must', u'VERB', u'aux', u'submit')\n",
      "(u'submit', u'VERB', u'ROOT', u'submit')\n",
      "(u'their', u'ADJ', u'poss', u'requests')\n",
      "(u'requests', u'NOUN', u'dobj', u'submit')\n",
      "(u'.', u'PUNCT', u'punct', u'submit')\n"
     ]
    }
   ],
   "source": [
    "apples = nlp(u'bright red apples on the tree')[2]\n",
    "print([w.text for w in apples.lefts])\n",
    "# ['bright', 'red']\n",
    "print([w.text for w in apples.rights])\n",
    "# ['on']\n",
    "#assert apples.n_lefts == 2\n",
    "#assert apples.n_rights == 1\n",
    "\n",
    "from spacy.symbols import nsubj\n",
    "doc = nlp(u'Credit and mortgage account holders must submit their requests within 30 days.')\n",
    "root = [w for w in doc if w.head is w][0]\n",
    "subject = list(root.lefts)[0]\n",
    "#for descendant in subject.subtree:\n",
    "    #assert subject.is_ancestor_of(descendant)\n",
    "\n",
    "from spacy.symbols import nsubj\n",
    "doc = nlp(u'Credit and mortgage account holders must submit their requests.')\n",
    "holders = doc[4]\n",
    "span = doc[holders.left_edge.i : holders.right_edge.i + 1]\n",
    "span.merge()\n",
    "for word in doc:\n",
    "    print(word.text, word.pos_, word.dep_, word.head.text)\n",
    "    # Credit and mortgage account holders nsubj NOUN submit\n",
    "    # must VERB aux submit\n",
    "    # submit VERB ROOT submit\n",
    "    # their DET det requests\n",
    "    # requests NOUN do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
