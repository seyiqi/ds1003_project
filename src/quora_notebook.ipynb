{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ONLY ON MY COMPUTER\n",
    "import sys\n",
    "import sys,os,os.path\n",
    "python_path = ['', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python27.zip', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-darwin', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac/lib-scriptpackages', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-tk', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-old', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg', '/Users/Melancardie/Dropbox/Documents/My School/NYU/Spring 2017/DS-GA 1008/HW/hw3/ALI']\n",
    "\n",
    "for p in python_path:\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DATA LOADING FUNCTIONS\n",
    "\n",
    "# split dataset\n",
    "def split_dataset(full_data, train_ratio, validation_ratio, test_ratio):\n",
    "    \"\"\"\n",
    "    Function that splits the dataset into train, validation, and test\n",
    "    \"\"\"\n",
    "    random_idx = np.random.permutation(len(full_data))\n",
    "    train_threshold = int(round(train_ratio*len(full_data)))\n",
    "    validation_threshold = int(round((train_ratio+validation_ratio)*len(full_data)))\n",
    "    \n",
    "    train_set = full_data.iloc[random_idx[:train_threshold]]\n",
    "    validation_set = full_data.iloc[random_idx[train_threshold:validation_threshold]]\n",
    "    test_set = full_data.iloc[random_idx[validation_threshold:]]\n",
    "    \n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "\n",
    "# load dataset\n",
    "def load_datasets(load_dir = \"../data/kaggle_competition/\", prefix=\"clean_kaggle_\", post_fix=\"\"):\n",
    "    \"\"\"\n",
    "    Function that loads the dataset\n",
    "    \"\"\"\n",
    "    train_set = pd.read_csv(os.path.join(load_dir, \"{0}train{1}.csv\".format(prefix,post_fix)), keep_default_na=False)\n",
    "    validation_set = pd.read_csv(os.path.join(load_dir, \"{0}validation{1}.csv\".format(prefix,post_fix)), keep_default_na=False)\n",
    "    test_set = pd.read_csv(os.path.join(load_dir, \"{0}test{1}.csv\".format(prefix,post_fix)), keep_default_na=False)\n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "def xy_split(df, label_col=\"is_duplicate\"):\n",
    "    \"\"\"\n",
    "    Function that splits a data frame into X and y\n",
    "    \"\"\"\n",
    "    return df.drop(label_col, axis=1).as_matrix(), df[label_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA CLEANING FUNCTIONS\n",
    "def clean_str(input_str):\n",
    "    \"\"\"\n",
    "    Helper function that converts string to ASCII\n",
    "    \"\"\"\n",
    "    # trivial case\n",
    "    if pd.isnull(input_str) or type(input_str)==np.float or type(input_str)==float:\n",
    "        return \"\"\n",
    "    # encoding\n",
    "    input_str = input_str.decode('ascii', 'ignore').lower()\n",
    "    return input_str\n",
    "\n",
    "def clean_dataset(full_dataset):\n",
    "    \"\"\"\n",
    "    Function that cleans the full dataset\n",
    "    \"\"\"\n",
    "    full_dataset[\"clean_q1\"] = full_dataset[\"question1\"].apply(clean_str,1)\n",
    "    full_dataset[\"clean_q2\"] = full_dataset[\"question2\"].apply(clean_str,1)\n",
    "    col_need = [\"clean_q1\", \"clean_q2\"]\n",
    "    if \"is_duplicate\" in full_dataset.columns:\n",
    "        col_need += [\"is_duplicate\"]\n",
    "    return full_dataset[col_need]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING FUNCTIONS\n",
    "def word_overlap(row):\n",
    "    \"\"\"\n",
    "    Function that calculates the percentage of word overlap\n",
    "    \"\"\"\n",
    "    avg_length = float(len(row['token_1'])+len(row['token_2']))/2\n",
    "    save_token_num = len(set(row['token_1']).intersection(set(row['token_2'])))\n",
    "    return float(save_token_num)/avg_length\n",
    "\n",
    "def sentence_similarity(row):\n",
    "    \"\"\"\n",
    "    Function that returns the Spacy sentence similarity\n",
    "    \"\"\"\n",
    "    return row[\"doc1\"].similarity(row[\"doc2\"])\n",
    "\n",
    "\n",
    "def jaccard_sim(set1, set2):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity\n",
    "    \"\"\"\n",
    "    if len(set1.union(set2)) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(len(set1.intersection(set2)))/len(set1.union(set2))\n",
    "\n",
    "def jaccard_sim_unhashbale(set1, set2):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity\n",
    "    \"\"\"\n",
    "    count = 0.0\n",
    "    str_set2 = str(set2)\n",
    "    for i in set1:\n",
    "        if str(i) in str_set2:\n",
    "            count += 1.0\n",
    "    if (len(set1)+len(set2)-count) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return count/ (len(set1)+len(set2)-count)\n",
    "    \n",
    "    \n",
    "def load_embedding(glove_file=\"/Users/Melancardie/Dropbox/Documents/My Research/NYU/Sundararajan/trust/lib/glove.6B/glove.6B.50d.txt\",\n",
    "                   line_to_load = 50000):\n",
    "    \"\"\"\n",
    "    Function that populates a dictionary with word embedding vectors\n",
    "    \"\"\"\n",
    "    ctr = 0\n",
    "    word_emb = {}\n",
    "    with open(glove_file, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            contents = line.split()\n",
    "            word_emb[contents[0]]=np.asarray(contents[1:]).astype(float)\n",
    "            ctr += 1\n",
    "            if ctr >= line_to_load:\n",
    "                break\n",
    "    return word_emb\n",
    "glove_emb = load_embedding()\n",
    " \n",
    "\n",
    "    \n",
    "def vectorize_tokens(token_list, word_emb, dim=50):\n",
    "    \"\"\"\n",
    "    Function that vectorize phrases from a counter\n",
    "    \"\"\"\n",
    "    ctr = 0.0\n",
    "    vec = np.zeros(dim)\n",
    "    for token in token_list:\n",
    "        if token in word_emb:\n",
    "            vec += word_emb[token].astype(float)\n",
    "            ctr += 1\n",
    "    if ctr == 0 :\n",
    "        return vec\n",
    "    else:\n",
    "        return vec / float(ctr)\n",
    "    \n",
    "def emb_dist(row, embedding):\n",
    "    \"\"\"\n",
    "    Function that calculates the euclidean distance among two embeddings\n",
    "    \"\"\"\n",
    "    # embedding\n",
    "    emb1 = vectorize_tokens(row[\"token_1\"], embedding)\n",
    "    emb2 = vectorize_tokens(row[\"token_2\"], embedding)\n",
    "    return np.linalg.norm(emb1-emb2)\n",
    "\n",
    "def emb_diff(row, embedding, emb_mat):\n",
    "    \"\"\"\n",
    "    Function that calculates the euclidean distance among two embeddings\n",
    "    \"\"\"\n",
    "    # embedding\n",
    "    emb1 = vectorize_tokens(row[\"token_1\"], embedding)\n",
    "    emb2 = vectorize_tokens(row[\"token_2\"], embedding)\n",
    "    emb_mat.append(np.abs(emb1-emb2))\n",
    "     \n",
    "\n",
    "def feature_engineering(df, embedding=glove_emb, normalize=False):\n",
    "    \"\"\"\n",
    "    Feature engineering function\n",
    "    \"\"\"\n",
    "    total_begin = time.time()\n",
    "    \n",
    "    # preprocessing #\n",
    "    # tokenization\n",
    "    df['token_1'] = df.apply(lambda x: nltk.word_tokenize(x[\"clean_q1\"]), 1)\n",
    "    df['token_2'] = df.apply(lambda x: nltk.word_tokenize(x[\"clean_q2\"]), 1)\n",
    "    # spacy rep\n",
    "    df['doc1'] = df.apply(lambda x: nlp(unicode(x[\"clean_q1\"], \"utf-8\")), 1)\n",
    "    df['doc2'] = df.apply(lambda x: nlp(unicode(x[\"clean_q2\"], \"utf-8\")), 1)\n",
    "    # capitalized spacy rep\n",
    "    df['cap_doc1'] = df.apply(lambda x: nlp(unicode(x[\"clean_q1\"].upper(), \"utf-8\")), 1)\n",
    "    df['cap_doc2'] = df.apply(lambda x: nlp(unicode(x[\"clean_q2\"].upper(), \"utf-8\")), 1)\n",
    "    # entity\n",
    "    df['entity_set_1'] = df.apply(lambda x: x[\"cap_doc1\"].ents, 1)\n",
    "    df['entity_set_2'] = df.apply(lambda x: x[\"cap_doc2\"].ents, 1)\n",
    "    # name chunk\n",
    "    df['noun_chunks_1'] = df.apply(lambda x: [chunk for chunk in x[\"cap_doc1\"].noun_chunks], 1)\n",
    "    df['noun_chunks_2'] = df.apply(lambda x: [chunk for chunk in x[\"cap_doc2\"].noun_chunks], 1)\n",
    "\n",
    "    preprocess_time = time.time()\n",
    "    print(\"preprocessed  for {0} seconds\".format(preprocess_time-total_begin))\n",
    "    \n",
    "    # length #\n",
    "    df.loc[:,\"len_1\"] = df.apply(lambda x: len(x[\"token_1\"]), 1)\n",
    "    df.loc[:,\"len_2\"] = df.apply(lambda x: len(x[\"token_2\"]), 1)\n",
    "    df.loc[:,\"len_diff\"] = np.abs(df[\"len_1\"]-df[\"len_2\"])\n",
    "    df.loc[:,\"len_diff_percent\"] = np.abs(df[\"len_1\"]-df[\"len_2\"]) /((df[\"len_1\"]+df[\"len_2\"])/2)\n",
    "    after_length = time.time()\n",
    "    print(\"length fueature loaded for {0} seconds\".format(after_length-preprocess_time))\n",
    "    \n",
    "    # first words match #\n",
    "    df.loc[:,\"first_word_q1\"] = df.apply(lambda x: x[\"clean_q1\"].split(\" \")[0], 1)\n",
    "    df.loc[:,\"first_word_q2\"] = df.apply(lambda x: x[\"clean_q2\"].split(\" \")[0], 1)\n",
    "    df.loc[:,\"first_word_match\"] = (df[\"first_word_q1\"] == df[\"first_word_q2\"])\n",
    "    after_first = time.time()\n",
    "    print(\"first word feature loaded for {0} seconds\".format(after_first-after_length))\n",
    "    \n",
    "    # bag of words #\n",
    "#     if tokenizer is None:\n",
    "#         bag_of_word_tokenizer = CountVectorizer(stop_words=\"english\", max_features=top_k_word)\n",
    "#     else:\n",
    "#         bag_of_word_tokenizer = tokenizer\n",
    "#     q1_matrix = bag_of_word_tokenizer.fit_transform(df[\"clean_q1\"]).astype(np.float)\n",
    "#     q2_matrix = bag_of_word_tokenizer.fit_transform(df[\"clean_q2\"]).astype(np.float)\n",
    "#     df[\"vec_q1\"] = [q1_matrix[i] for i in range(len(df))]\n",
    "#     df[\"vec_q2\"] = [q2_matrix[i] for i in range(len(df))]\n",
    "#     print(\"question vectorized\")\n",
    "\n",
    "    \n",
    "    # similarity measure #\n",
    "    #cosine_sim = [cosine_similarity(q1_matrix[i], q2_matrix[i])[0][0] for i in range(len(df))]\n",
    "    #df[\"cosine_sim\"] = cosine_sim\n",
    "    df.loc[:,\"overlap_percent\"] = df.apply(word_overlap, 1)\n",
    "    # Spacy stentence similarity\n",
    "    df.loc[:,\"spacy_sentence_similarity\"] = df.apply(sentence_similarity, 1)\n",
    "    # edit distance\n",
    "    df.loc[:,\"edit_distance\"] = df.apply(lambda x: nltk.edit_distance(x[\"token_1\"], x[\"token_2\"]), 1)\n",
    "    # token Jaccard\n",
    "    df.loc[:,\"token_jaccard\"] = df.apply(lambda x: jaccard_sim(set(x[\"token_1\"]), set(x[\"token_2\"])), 1)\n",
    "    after_sim = time.time()\n",
    "    print(\"similarity feature loaded for {0} seconds\".format(after_sim-after_first))\n",
    "    \n",
    "    # embedding #\n",
    "    # embedding diff -- UGLY\n",
    "    dim_emb = embedding.values()[0].shape[0]\n",
    "    emb_mat = []\n",
    "    df.apply(lambda x: emb_diff(x, embedding, emb_mat), 1)\n",
    "    emb_mat = np.array(emb_mat)\n",
    "    for dim in range(dim_emb):\n",
    "        df[\"emb_diff_dim_{0}\".format(dim)] = emb_mat[:,dim]\n",
    "    # euclidean distance - embedding\n",
    "    df.loc[:,\"emb_dist\"] = df.apply(lambda x: emb_dist(x, embedding), 1)\n",
    "    after_emb = time.time()\n",
    "    print(\"embedding feature loaded for {0} seconds\".format(after_emb-after_sim))\n",
    "    \n",
    "    # entity features #\n",
    "    # entity same\n",
    "    df.loc[:,\"entity_same\"] = df.apply(lambda x: x[\"entity_set_1\"]==x[\"entity_set_2\"], 1)\n",
    "    # entity # same\n",
    "    df.loc[:,\"entity_len_same\"] = df.apply(lambda x: len(x[\"entity_set_1\"])==len(x[\"entity_set_2\"]), 1)\n",
    "    # entity # diff\n",
    "    df.loc[:,\"entity_len_diff\"] = df.apply(lambda x: np.abs(len(x[\"entity_set_1\"])-len(x[\"entity_set_2\"])), 1)\n",
    "    # entity Jaccard\n",
    "    df.loc[:,\"entity_jaccard\"] = df.apply(lambda x: jaccard_sim_unhashbale(x[\"entity_set_1\"], x[\"entity_set_2\"]), 1)\n",
    "    \n",
    "    # noun chunk same\n",
    "    df.loc[:, \"chunk_same\"] = df.apply(lambda x: x[\"noun_chunks_1\"]==x[\"noun_chunks_2\"], 1)\n",
    "    # noun chunk # same\n",
    "    df.loc[:,\"chunk_len_same\"] = df.apply(lambda x: len(x[\"noun_chunks_1\"])==len(x[\"noun_chunks_2\"]), 1)\n",
    "    # noun chunk # diff\n",
    "    df.loc[:,\"chunk_len_diff\"] = df.apply(lambda x: np.abs(len(x[\"noun_chunks_1\"])-len(x[\"noun_chunks_2\"])), 1)\n",
    "    # noun chunk Jaccard\n",
    "    df.loc[:,\"chunk_jaccard\"] = df.apply(lambda x: jaccard_sim_unhashbale(x[\"noun_chunks_1\"], x[\"noun_chunks_2\"]), 1)\n",
    "    after_entity = time.time()\n",
    "    print(\"entity feature loaded for {0} seconds\".format(after_entity-after_emb))\n",
    "    \n",
    "    \n",
    "    # filter columns\n",
    "    ignore_columns = [\"first_word_q1\", \"first_word_q2\", \"clean_q1\", \"clean_q2\", \"token_1\", \"token_2\", \n",
    "                      \"doc1\", \"doc2\", \"cap_doc1\", \"cap_doc2\", \"noun_chunks_1\", \"noun_chunks_2\",\n",
    "                     \"entity_set_1\", \"entity_set_2\"]\n",
    "    col_normalize = ['len_1', 'len_2', 'len_diff', 'edit_distance', 'emb_dist', 'entity_len_diff', 'chunk_len_diff']\n",
    "    #full_feature_df = df\n",
    "    clean_feature_df = df.drop(ignore_columns, axis=1)\n",
    "    if normalize:\n",
    "        for col in clean_feature_df.columns:\n",
    "            if str(col) in col_normalize:\n",
    "                col_max = np.max(clean_feature_df[col])\n",
    "                col_min = np.min(clean_feature_df[col])\n",
    "                clean_feature_df[col] = (clean_feature_df[col]-col_min)/float(col_max-col_min)\n",
    "    after_normalize = time.time()\n",
    "    print(\"normalization time = {0}\".format(time.time()-after_normalize))\n",
    "    print(\"total time = {0}\".format(time.time()-total_begin))\n",
    "    return clean_feature_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DATA CREATION SCRIPTS\n",
    "# Quora Dataset\n",
    "#full_data = pd.read_csv(\"../data/questions.csv\")\n",
    "# Kaggle Dataset\n",
    "# begin_time = time.time()\n",
    "# kaggle_train = pd.read_csv(\"../data/kaggle_competition/origin/train.csv\")\n",
    "# kaggle_test = pd.read_csv(\"../data/kaggle_competition/origin/test.csv\")\n",
    "# print(\"data loaded, used {0} seconds\".format(time.time()-begin_time))\n",
    "\n",
    "# clean dataset\n",
    "# begin_time = time.time()\n",
    "# clean_train = clean_dataset(kaggle_train)\n",
    "# clean_test = clean_dataset(kaggle_test)\n",
    "#clean_train.to_csv(\"../data/kaggle_competition/clean_datasets/clean_train.csv\", index=False)\n",
    "#clean_test.to_csv(\"../data/kaggle_competition/clean_datasets/clean_test.csv\", index=False)\n",
    "#print(\"data cleaned, used {0} seconds\".format(time.time()-begin_time))\n",
    "\n",
    "\n",
    "# split and save dataset\n",
    "#begin_time = time.time()\n",
    "# since Kaggle has its own test set, test_ratio=0\n",
    "# train_set, validation_set, _ = split_dataset(clean_train, 0.8, 0.2, 0)\n",
    "# test_set = clean_test\n",
    "# train_set.to_csv(\"../data/kaggle_competition/clean_kaggle_train.csv\", index=False)\n",
    "# validation_set.to_csv(\"../data/kaggle_competition/clean_kaggle_validation.csv\", index=False)\n",
    "# test_set.to_csv(\"../data/kaggle_competition/clean_kaggle_test.csv\", index=False)\n",
    "# load splitted dataset\n",
    "#train_set, validation_set, test_set = load_datasets()\n",
    "# print(\"data splitted, used {0} seconds\".format(time.time()-begin_time))\n",
    "\n",
    "\n",
    "# feature engineering\n",
    "#begin_time = time.time()\n",
    "#feature_train = feature_engineering(train_set)\n",
    "#feature_train.to_csv(\"../data/kaggle_competition/feature_datasets/feature_train_v2.csv\", index=False)\n",
    "#feature_validation = feature_engineering(validation_set)\n",
    "#feature_validation.to_csv(\"../data/kaggle_competition/feature_datasets/feature_validation_v2.csv\", index=False)\n",
    "#feature_test = feature_engineering(test_set)\n",
    "#feature_test.to_csv(\"../data/kaggle_competition/feature_datasets/feature_test_v2.csv\", index=False)\n",
    "#print(\"data featurized, used {0} seconds\".format(time.time()-begin_time))\n",
    "#feature_train, feature_validation, feature_test= load_datasets(load_dir = \"../data/kaggle_competition/feature_datasets\", prefix=\"feature_\", post_fix=\"_v2\")\n",
    "# load splitted dataset\n",
    "# train_set, validation_set, test_set = load_datasets()\n",
    "\n",
    "# split X, y\n",
    "X_train, y_train = xy_split(feature_train)\n",
    "X_validate, y_validate = xy_split(feature_validation)\n",
    "#X_test=feature_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323432\n",
      "80858\n",
      "2345796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Melancardie/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Melancardie/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Melancardie/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Melancardie/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Melancardie/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Melancardie/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Melancardie/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:111: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Melancardie/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Melancardie/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Melancardie/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed  for 4.30770587921 seconds\n",
      "length fueature loaded for 0.612040996552 seconds\n",
      "first word feature loaded for 0.460093021393 seconds\n",
      "similarity feature loaded for 1.04951405525 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Melancardie/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:167: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding feature loaded for 3.29013299942 seconds\n",
      "entity feature loaded for 1.28866410255 seconds\n",
      "normalization time = 0.0\n",
      "total time = 11.0092570782\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(validation_set))\n",
    "print(len(test_set))\n",
    "feature_train = feature_engineering(train_set.head(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>len_1</th>\n",
       "      <th>len_2</th>\n",
       "      <th>len_diff</th>\n",
       "      <th>len_diff_percent</th>\n",
       "      <th>first_word_match</th>\n",
       "      <th>overlap_percent</th>\n",
       "      <th>spacy_sentence_similarity</th>\n",
       "      <th>edit_distance</th>\n",
       "      <th>token_jaccard</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_diff_dim_49</th>\n",
       "      <th>emb_dist</th>\n",
       "      <th>entity_same</th>\n",
       "      <th>entity_len_same</th>\n",
       "      <th>entity_len_diff</th>\n",
       "      <th>entity_jaccard</th>\n",
       "      <th>chunk_same</th>\n",
       "      <th>chunk_len_same</th>\n",
       "      <th>chunk_len_diff</th>\n",
       "      <th>chunk_jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>True</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.972389</td>\n",
       "      <td>8</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083625</td>\n",
       "      <td>0.565029</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>True</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.967301</td>\n",
       "      <td>2</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016534</td>\n",
       "      <td>0.738428</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.987928</td>\n",
       "      <td>1</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151677</td>\n",
       "      <td>0.489465</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>False</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.942117</td>\n",
       "      <td>6</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345644</td>\n",
       "      <td>0.899277</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>21</td>\n",
       "      <td>1.135135</td>\n",
       "      <td>False</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.768640</td>\n",
       "      <td>27</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072233</td>\n",
       "      <td>2.038899</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate  len_1  len_2  len_diff  len_diff_percent first_word_match  \\\n",
       "0             1     14     17         3          0.193548             True   \n",
       "1             1      6      7         1          0.153846             True   \n",
       "2             1      8      8         0          0.000000            False   \n",
       "3             0      9     10         1          0.105263            False   \n",
       "4             0      8     29        21          1.135135            False   \n",
       "\n",
       "   overlap_percent  spacy_sentence_similarity  edit_distance  token_jaccard  \\\n",
       "0         0.645161                   0.972389              8       0.476190   \n",
       "1         0.769231                   0.967301              2       0.625000   \n",
       "2         0.875000                   0.987928              1       0.777778   \n",
       "3         0.421053                   0.942117              6       0.266667   \n",
       "4         0.108108                   0.768640             27       0.064516   \n",
       "\n",
       "       ...        emb_diff_dim_49  emb_dist  entity_same  entity_len_same  \\\n",
       "0      ...               0.083625  0.565029         True             True   \n",
       "1      ...               0.016534  0.738428         True             True   \n",
       "2      ...               0.151677  0.489465        False             True   \n",
       "3      ...               0.345644  0.899277        False             True   \n",
       "4      ...               0.072233  2.038899        False            False   \n",
       "\n",
       "   entity_len_diff  entity_jaccard  chunk_same  chunk_len_same  \\\n",
       "0                0               0       False           False   \n",
       "1                0               0       False           False   \n",
       "2                0               0       False           False   \n",
       "3                0               0       False           False   \n",
       "4                3               0       False           False   \n",
       "\n",
       "   chunk_len_diff  chunk_jaccard  \n",
       "0               2       0.142857  \n",
       "1               2       0.000000  \n",
       "2               1       0.500000  \n",
       "3               2       0.000000  \n",
       "4               5       0.000000  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(323432, 6)\n",
      "(323432,)\n",
      "(80858, 6)\n",
      "(80858,)\n",
      "(2345796, 6)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print y_train.shape\n",
    "print X_validate.shape\n",
    "print y_validate.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65178520641719695"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mat1 = full_feature_df[\"vec_q1\"].iloc[0]\n",
    "#mat2 = full_feature_df[\"vec_q2\"].iloc[0]\n",
    "\n",
    "#cosine_similarity(mat1[5], mat2[5])\n",
    "#a = time.time()\n",
    "#cosine_similarity(full_feature_df[\"vec_q1\"].iloc[10],full_feature_df[\"vec_q2\"].iloc[10] )\n",
    "#print time.time()-a\n",
    "#len(full_feature_df)\n",
    "#full_feature_df.to_csv(\"../data/full_feature_df.csv\", index=False)\n",
    "#np.sum(clean_feature_df[\"cosine_sim\"]==0)/float(len(clean_feature_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mat1 = mat1.astype(np.float)\n",
    "#mat2 = mat2.astype(np.float)\n",
    "#cosine_similarity(mat1, mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PREDICTIVE MODEL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MODEL ANALYTICS FUNCTIONS\n",
    "def all_test_metrics(y_pred, y_test, metrics_list=[\"acc\", \"auc\", \"f1\", \"nll\"]):\n",
    "    score_dict = {}\n",
    "    # acc\n",
    "    if \"acc\" in metrics_list:\n",
    "        y_pred_acc = np.round(y_pred).astype(np.int8)\n",
    "        acc = metrics.accuracy_score(y_test, y_pred_acc, normalize=True)\n",
    "        score_dict[\"acc\"] = acc \n",
    "    # auc\n",
    "    if \"auc\" in metrics_list:\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        score_dict[\"auc\"] = auc\n",
    "        #score_dict[\"fpr\"] = fpr\n",
    "        #score_dict[\"tpr\"] = tpr\n",
    "    # f1-measure\n",
    "    if \"f1\" in metrics_list:\n",
    "        y_pred_acc = np.round(y_pred).astype(np.int8)\n",
    "        f1 = metrics.f1_score(y_test, y_pred_acc, labels=[0,1], pos_label=1)\n",
    "        score_dict[\"f1\"] = f1\n",
    "    # nll\n",
    "    if \"nll\" in metrics_list:\n",
    "        nll = metrics.log_loss(y_test, y_pred)\n",
    "        score_dict[\"nll\"] = nll\n",
    "    return score_dict\n",
    "\n",
    "\n",
    "def test_model(model, X_test, y_test, verbose=True, model_name=\"\", y_pred_test=None, pred_lambda=None):\n",
    "    \"\"\"\n",
    "    Function that generate performance stats for a model\n",
    "    \"\"\"\n",
    "    if y_pred_test is None:\n",
    "        if pred_lambda is None:\n",
    "            y_pred_test = model.predict(X_test)\n",
    "        else:\n",
    "            y_pred_test = pred_lambda(model, X_test)  \n",
    "    scores = all_test_metrics(y_pred_test, y_test)    \n",
    "    if verbose:\n",
    "        print(model_name+\":\")\n",
    "        print(scores)\n",
    "    return y_pred_test, scores\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline 1 - Majority Class (Validation)::\n",
      "{'acc': 0.63063642434885847, 'f1': 0.0, 'auc': 0.5, 'nll': 12.757365947839453}\n",
      "Baseline 2 - Simple Word Overlap (Validation)::\n",
      "{'acc': 0.66991515990996564, 'f1': 0.61955127291387524, 'auc': 0.73065771989901296, 'nll': 0.66146289432171168}\n",
      "Baseline 3 - Simple Logistic Regression (Validation)::\n",
      "{'acc': 0.68349452125949195, 'f1': 0.55166252058442244, 'auc': 0.75714678542401015, 'nll': 0.54561463901481544}\n"
     ]
    }
   ],
   "source": [
    "# MODEL ANALYTICS SCRIPT\n",
    "n_valid = len(validation_set)\n",
    "n_test = len(test_set)\n",
    "\n",
    "# baseline 1: majority class\n",
    "y_pred_valid = [0 for i in range(n_valid)]\n",
    "#y_pred_test = [0 for i in range(n_test)]\n",
    "_, score_majority_class_valid = test_model(None, None, y_validate, \n",
    "                                        verbose=True, model_name=\"Baseline 1 - Majority Class (Validation):\",\n",
    "                                        y_pred_test=y_pred_valid)\n",
    "\n",
    "# baseline 2: simple word overlap\n",
    "y_pred_valid = X_validate[:,5].astype(np.double)\n",
    "#y_pred_test = X_test[:,5].astype(np.double)\n",
    "_, score_majority_class_valid = test_model(None, None, y_validate, \n",
    "                                        verbose=True, model_name=\"Baseline 2 - Simple Word Overlap (Validation):\",\n",
    "                                        y_pred_test=y_pred_valid)\n",
    "\n",
    "# baseline 3: logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr_lambda = lambda model, x: model.predict_proba(x)[:,1]\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred_valid, score_majority_class_valid = test_model(lr, X_validate, y_validate, verbose=True,\n",
    "                                                       model_name=\"Baseline 3 - Simple Logistic Regression (Validation):\",\n",
    "                                                      pred_lambda=lr_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until valid error hasn't decreased in 50 rounds.\n",
      "[0]\ttrain-logloss:0.667396\tvalid-logloss:0.668245\n",
      "[1]\ttrain-logloss:0.647427\tvalid-logloss:0.647754\n",
      "[2]\ttrain-logloss:0.630885\tvalid-logloss:0.630666\n",
      "[3]\ttrain-logloss:0.616394\tvalid-logloss:0.616429\n",
      "[4]\ttrain-logloss:0.604267\tvalid-logloss:0.604557\n",
      "[5]\ttrain-logloss:0.593869\tvalid-logloss:0.594216\n",
      "[6]\ttrain-logloss:0.585130\tvalid-logloss:0.584907\n",
      "[7]\ttrain-logloss:0.577106\tvalid-logloss:0.577148\n",
      "[8]\ttrain-logloss:0.570334\tvalid-logloss:0.570522\n",
      "[9]\ttrain-logloss:0.564192\tvalid-logloss:0.564602\n",
      "[10]\ttrain-logloss:0.558790\tvalid-logloss:0.559089\n",
      "[11]\ttrain-logloss:0.554301\tvalid-logloss:0.554600\n",
      "[12]\ttrain-logloss:0.550374\tvalid-logloss:0.550688\n",
      "[13]\ttrain-logloss:0.546638\tvalid-logloss:0.547129\n",
      "[14]\ttrain-logloss:0.543676\tvalid-logloss:0.544075\n",
      "[15]\ttrain-logloss:0.541156\tvalid-logloss:0.541324\n",
      "[16]\ttrain-logloss:0.538388\tvalid-logloss:0.538522\n",
      "[17]\ttrain-logloss:0.535970\tvalid-logloss:0.536354\n",
      "[18]\ttrain-logloss:0.534203\tvalid-logloss:0.534248\n",
      "[19]\ttrain-logloss:0.532175\tvalid-logloss:0.532499\n",
      "[20]\ttrain-logloss:0.530690\tvalid-logloss:0.530998\n",
      "[21]\ttrain-logloss:0.529286\tvalid-logloss:0.529556\n",
      "[22]\ttrain-logloss:0.527692\tvalid-logloss:0.528267\n",
      "[23]\ttrain-logloss:0.526740\tvalid-logloss:0.526863\n",
      "[24]\ttrain-logloss:0.525670\tvalid-logloss:0.525822\n",
      "[25]\ttrain-logloss:0.524530\tvalid-logloss:0.524845\n",
      "[26]\ttrain-logloss:0.523322\tvalid-logloss:0.523975\n",
      "[27]\ttrain-logloss:0.522545\tvalid-logloss:0.523028\n",
      "[28]\ttrain-logloss:0.522020\tvalid-logloss:0.522206\n",
      "[29]\ttrain-logloss:0.521203\tvalid-logloss:0.521586\n",
      "[30]\ttrain-logloss:0.520242\tvalid-logloss:0.520814\n",
      "[31]\ttrain-logloss:0.519747\tvalid-logloss:0.520098\n",
      "[32]\ttrain-logloss:0.519161\tvalid-logloss:0.519608\n",
      "[33]\ttrain-logloss:0.518719\tvalid-logloss:0.519187\n",
      "[34]\ttrain-logloss:0.518337\tvalid-logloss:0.518822\n",
      "[35]\ttrain-logloss:0.518033\tvalid-logloss:0.518430\n",
      "[36]\ttrain-logloss:0.517748\tvalid-logloss:0.517996\n",
      "[37]\ttrain-logloss:0.516928\tvalid-logloss:0.517595\n",
      "[38]\ttrain-logloss:0.516573\tvalid-logloss:0.517171\n",
      "[39]\ttrain-logloss:0.516251\tvalid-logloss:0.516806\n",
      "[40]\ttrain-logloss:0.515689\tvalid-logloss:0.516190\n",
      "[41]\ttrain-logloss:0.515056\tvalid-logloss:0.515815\n",
      "[42]\ttrain-logloss:0.514832\tvalid-logloss:0.515589\n",
      "[43]\ttrain-logloss:0.514605\tvalid-logloss:0.515333\n",
      "[44]\ttrain-logloss:0.514333\tvalid-logloss:0.515069\n",
      "[45]\ttrain-logloss:0.514137\tvalid-logloss:0.514893\n",
      "[46]\ttrain-logloss:0.513855\tvalid-logloss:0.514627\n",
      "[47]\ttrain-logloss:0.513657\tvalid-logloss:0.514476\n",
      "[48]\ttrain-logloss:0.513367\tvalid-logloss:0.514344\n",
      "[49]\ttrain-logloss:0.513142\tvalid-logloss:0.514169\n",
      "[50]\ttrain-logloss:0.512609\tvalid-logloss:0.513666\n",
      "[51]\ttrain-logloss:0.512250\tvalid-logloss:0.513370\n"
     ]
    }
   ],
   "source": [
    "#XG BOOST\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.1\n",
    "params['max_depth'] = 4\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(X_validate, label=y_validate)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add single entity feature \n",
    "def upper_str(input_str):\n",
    "    input_str = input_str.upper()\n",
    "    return input_str\n",
    "\n",
    "def upper_dataset(full_dataset):\n",
    "    \"\"\"\n",
    "    Function that cleans the full dataset\n",
    "    \"\"\"\n",
    "    full_dataset[\"clean_q1\"] = full_dataset[\"clean_q1\"].apply(upper_str,1)\n",
    "    full_dataset[\"clean_q2\"] = full_dataset[\"clean_q2\"].apply(upper_str,1)\n",
    "    return full_dataset\n",
    "\n",
    "X_train=upper_dataset(X_train)\n",
    "X_valid=upper_dataset(X_validate)\n",
    "X_test=upper_dataset(X_test)\n",
    "\n",
    "def singleentity(row):\n",
    "    \"\"\"\n",
    "    Function that calculates where there is any entity difference between the two questions\n",
    "    \"\"\"\n",
    "    doc_1 = nlp(row[\"clean_q1\"])\n",
    "    doc_2 = nlp(row[\"clean_q2\"])\n",
    "    if len(doc_1.ents)!=len(doc_2.ents):\n",
    "        return 1\n",
    "    else:\n",
    "        for ent in doc_2.ents:\n",
    "            if ent not in doc_1.ents:\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "X_train[\"singleentity\"]=X_train.apply(singleentity,1)\n",
    "X_valid[\"singleentity\"]=X_valid.apply(singleentity,1)\n",
    "X_test[\"singleentity\"]=X_test.apply(singleentity,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add edit distance feature\n",
    "def distance(row):\n",
    "    \"\"\"\n",
    "    Function that calculates the percentage of edit distance over the average length\n",
    "    \"\"\"\n",
    "    token_1 = nltk.word_tokenize(row[\"clean_q1\"])\n",
    "    token_2 = nltk.word_tokenize(row[\"clean_q2\"])\n",
    "    avg_length = float(len(token_1)+len(token_2))/2\n",
    "    return float(nltk.edit_distance(token_1,token_2))/avg_length\n",
    "\n",
    "X_train[\"Edit_distance\"]=X_train.apply(distance,1)\n",
    "X_valid[\"Edit_distance\"]=X_valid.apply(distance,1)\n",
    "X_test[\"Edit_distance\"]=X_test.apply(distance,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add % of length of the longest common sequence\n",
    "#### May not use as it takes a long time to run ####\n",
    "def lcs(xstr, ystr):\n",
    "    if not xstr or not ystr:\n",
    "        return 0\n",
    "    x, xs, y, ys = xstr[0], xstr[1:], ystr[0], ystr[1:]\n",
    "    if x == y:\n",
    "        return 1 + lcs(xs, ys)\n",
    "    else:\n",
    "        return max(lcs(xstr, ys), lcs(xs, ystr))\n",
    "\n",
    "def longestcommonsequence(row):\n",
    "    token_1 = nltk.word_tokenize(row[\"clean_q1\"])\n",
    "    token_2 = nltk.word_tokenize(row[\"clean_q2\"])\n",
    "    avg_length = float(len(token_1)+len(token_2))/2\n",
    "    return lcs(token_1,token_2)/avg_length  \n",
    "\n",
    "X_train[\"LCS\"]=X_train.apply(longestcommonsequence,1)\n",
    "X_valid[\"LCS\"]=X_valid.apply(longestcommonsequence,1)\n",
    "X_test[\"LCS\"]=X_test.apply(longestcommonsequence,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add similarity score \n",
    "def similarity(row):\n",
    "    sent_1 = nlp(row[\"clean_q1\"])\n",
    "    sent_2 = nlp(row[\"clean_q2\"])\n",
    "    return sent_1.similarity(sent_2)  \n",
    "\n",
    "X_train[\"Similarity\"]=X_train.apply(similarity,1)\n",
    "X_valid[\"Similarity\"]=X_valid.apply(similarity,1)\n",
    "X_test[\"Similarity\"]=X_test.apply(similarity,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#XG BOOST\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 4\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(X_validate, label=y_validate)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 2000, watchlist, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
