{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ONLY ON MY COMPUTER\n",
    "import sys\n",
    "import sys,os,os.path\n",
    "python_path = ['', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python27.zip', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-darwin', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac/lib-scriptpackages', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-tk', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-old', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages', '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg', '/Users/Melancardie/Dropbox/Documents/My School/NYU/Spring 2017/DS-GA 1008/HW/hw3/ALI']\n",
    "\n",
    "for p in python_path:\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DATA LOADING FUNCTIONS\n",
    "\n",
    "# split dataset\n",
    "def split_dataset(full_data, train_ratio, validation_ratio, test_ratio):\n",
    "    \"\"\"\n",
    "    Function that splits the dataset into train, validation, and test\n",
    "    \"\"\"\n",
    "    random_idx = np.random.permutation(len(full_data))\n",
    "    train_threshold = int(round(train_ratio*len(full_data)))\n",
    "    validation_threshold = int(round((train_ratio+validation_ratio)*len(full_data)))\n",
    "    \n",
    "    train_set = full_data.iloc[random_idx[:train_threshold]]\n",
    "    validation_set = full_data.iloc[random_idx[train_threshold:validation_threshold]]\n",
    "    test_set = full_data.iloc[random_idx[validation_threshold:]]\n",
    "    \n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "\n",
    "# load dataset\n",
    "def load_datasets(load_dir = \"../data/kaggle_competition/\", prefix=\"clean_kaggle_\", post_fix=\"\"):\n",
    "    \"\"\"\n",
    "    Function that loads the dataset\n",
    "    \"\"\"\n",
    "    train_set = pd.read_csv(os.path.join(load_dir, \"{0}train{1}.csv\".format(prefix,post_fix)), keep_default_na=False)\n",
    "    validation_set = pd.read_csv(os.path.join(load_dir, \"{0}validation{1}.csv\".format(prefix,post_fix)), keep_default_na=False)\n",
    "    test_set = pd.read_csv(os.path.join(load_dir, \"{0}test{1}.csv\".format(prefix,post_fix)), keep_default_na=False)\n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "def xy_split(df, label_col=\"is_duplicate\"):\n",
    "    \"\"\"\n",
    "    Function that splits a data frame into X and y\n",
    "    \"\"\"\n",
    "    return df.drop(label_col, axis=1).as_matrix(), df[label_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA CLEANING FUNCTIONS\n",
    "def clean_str(input_str):\n",
    "    \"\"\"\n",
    "    Helper function that converts string to ASCII\n",
    "    \"\"\"\n",
    "    # trivial case\n",
    "    if pd.isnull(input_str) or type(input_str)==np.float or type(input_str)==float:\n",
    "        return \"\"\n",
    "    # encoding\n",
    "    input_str = input_str.decode('ascii', 'ignore').lower()\n",
    "    # remove bad symbols\n",
    "    input_str = input_str.replace(\"\\n\",\"\")\n",
    "    return input_str\n",
    "\n",
    "def clean_dataset(full_dataset):\n",
    "    \"\"\"\n",
    "    Function that cleans the full dataset\n",
    "    \"\"\"\n",
    "    full_dataset[\"clean_q1\"] = full_dataset[\"question1\"].apply(clean_str,1)\n",
    "    full_dataset[\"clean_q2\"] = full_dataset[\"question2\"].apply(clean_str,1)\n",
    "    col_need = [\"clean_q1\", \"clean_q2\"]\n",
    "    if \"is_duplicate\" in full_dataset.columns:\n",
    "        col_need += [\"is_duplicate\"]\n",
    "    return full_dataset[col_need]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING FUNCTIONS\n",
    "def word_overlap(row):\n",
    "    \"\"\"\n",
    "    Function that calculates the percentage of word overlap\n",
    "    \"\"\"\n",
    "    avg_length = float(len(row['token_1'])+len(row['token_2']))/2\n",
    "    save_token_num = len(set(row['token_1']).intersection(set(row['token_2'])))\n",
    "    return float(save_token_num)/avg_length\n",
    "\n",
    "def sentence_similarity(row):\n",
    "    \"\"\"\n",
    "    Function that returns the Spacy sentence similarity\n",
    "    \"\"\"\n",
    "    return row[\"doc1\"].similarity(row[\"doc2\"])\n",
    "\n",
    "\n",
    "def jaccard_sim(set1, set2):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity\n",
    "    \"\"\"\n",
    "    if len(set1.union(set2)) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(len(set1.intersection(set2)))/len(set1.union(set2))\n",
    "\n",
    "def jaccard_sim_unhashbale(set1, set2):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity\n",
    "    \"\"\"\n",
    "    count = 0.0\n",
    "    str_set2 = str(set2)\n",
    "    for i in set1:\n",
    "        if str(i) in str_set2:\n",
    "            count += 1.0\n",
    "    if (len(set1)+len(set2)-count) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return count/ (len(set1)+len(set2)-count)\n",
    "    \n",
    "    \n",
    "def load_embedding(glove_file=\"/Users/Melancardie/Dropbox/Documents/My Research/NYU/Sundararajan/trust/lib/glove.6B/glove.6B.50d.txt\",\n",
    "                   line_to_load = 50000):\n",
    "    \"\"\"\n",
    "    Function that populates a dictionary with word embedding vectors\n",
    "    \"\"\"\n",
    "    ctr = 0\n",
    "    word_emb = {}\n",
    "    with open(glove_file, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            contents = line.split()\n",
    "            word_emb[contents[0]]=np.asarray(contents[1:]).astype(float)\n",
    "            ctr += 1\n",
    "            if ctr >= line_to_load:\n",
    "                break\n",
    "    return word_emb\n",
    "glove_emb = load_embedding()\n",
    " \n",
    "\n",
    "    \n",
    "def vectorize_tokens(token_list, word_emb, dim=50):\n",
    "    \"\"\"\n",
    "    Function that vectorize phrases from a counter\n",
    "    \"\"\"\n",
    "    ctr = 0.0\n",
    "    vec = np.zeros(dim)\n",
    "    for token in token_list:\n",
    "        if token in word_emb:\n",
    "            vec += word_emb[token].astype(float)\n",
    "            ctr += 1\n",
    "    if ctr == 0 :\n",
    "        return vec\n",
    "    else:\n",
    "        return vec / float(ctr)\n",
    "    \n",
    "def emb_dist(row, embedding):\n",
    "    \"\"\"\n",
    "    Function that calculates the euclidean distance among two embeddings\n",
    "    \"\"\"\n",
    "    # embedding\n",
    "    emb1 = vectorize_tokens(row[\"token_1\"], embedding)\n",
    "    emb2 = vectorize_tokens(row[\"token_2\"], embedding)\n",
    "    return np.linalg.norm(emb1-emb2)\n",
    "\n",
    "def emb_diff(row, embedding, emb_mat):\n",
    "    \"\"\"\n",
    "    Function that calculates the euclidean distance among two embeddings\n",
    "    \"\"\"\n",
    "    # embedding\n",
    "    emb1 = vectorize_tokens(row[\"token_1\"], embedding)\n",
    "    emb2 = vectorize_tokens(row[\"token_2\"], embedding)\n",
    "    emb_mat.append(np.abs(emb1-emb2))\n",
    "     \n",
    "\n",
    "def feature_engineering(df, embedding=glove_emb, normalize=False):\n",
    "    \"\"\"\n",
    "    Feature engineering function\n",
    "    \"\"\"\n",
    "    total_begin = time.time()\n",
    "    \n",
    "    # preprocessing #\n",
    "    # tokenization\n",
    "    df['token_1'] = df.apply(lambda x: nltk.word_tokenize(x[\"clean_q1\"]), 1)\n",
    "    df['token_2'] = df.apply(lambda x: nltk.word_tokenize(x[\"clean_q2\"]), 1)\n",
    "    # spacy rep\n",
    "    df['doc1'] = df.apply(lambda x: nlp(unicode(x[\"clean_q1\"], \"utf-8\")), 1)\n",
    "    df['doc2'] = df.apply(lambda x: nlp(unicode(x[\"clean_q2\"], \"utf-8\")), 1)\n",
    "    # capitalized spacy rep\n",
    "    df['cap_doc1'] = df.apply(lambda x: nlp(unicode(x[\"clean_q1\"].upper(), \"utf-8\")), 1)\n",
    "    df['cap_doc2'] = df.apply(lambda x: nlp(unicode(x[\"clean_q2\"].upper(), \"utf-8\")), 1)\n",
    "    # entity\n",
    "    df['entity_set_1'] = df.apply(lambda x: x[\"cap_doc1\"].ents, 1)\n",
    "    df['entity_set_2'] = df.apply(lambda x: x[\"cap_doc2\"].ents, 1)\n",
    "    # name chunk\n",
    "    df['noun_chunks_1'] = df.apply(lambda x: [chunk for chunk in x[\"cap_doc1\"].noun_chunks], 1)\n",
    "    df['noun_chunks_2'] = df.apply(lambda x: [chunk for chunk in x[\"cap_doc2\"].noun_chunks], 1)\n",
    "\n",
    "    preprocess_time = time.time()\n",
    "    print(\"preprocessed  for {0} seconds\".format(preprocess_time-total_begin))\n",
    "    \n",
    "    # length #\n",
    "    df.loc[:,\"len_1\"] = df.apply(lambda x: len(x[\"token_1\"]), 1)\n",
    "    df.loc[:,\"len_2\"] = df.apply(lambda x: len(x[\"token_2\"]), 1)\n",
    "    df.loc[:,\"len_diff\"] = np.abs(df[\"len_1\"]-df[\"len_2\"])\n",
    "    df.loc[:,\"len_diff_percent\"] = np.abs(df[\"len_1\"]-df[\"len_2\"]) /((df[\"len_1\"]+df[\"len_2\"])/2)\n",
    "    after_length = time.time()\n",
    "    print(\"length fueature loaded for {0} seconds\".format(after_length-preprocess_time))\n",
    "    \n",
    "    # first words match #\n",
    "    df.loc[:,\"first_word_q1\"] = df.apply(lambda x: x[\"clean_q1\"].split(\" \")[0], 1)\n",
    "    df.loc[:,\"first_word_q2\"] = df.apply(lambda x: x[\"clean_q2\"].split(\" \")[0], 1)\n",
    "    df.loc[:,\"first_word_match\"] = (df[\"first_word_q1\"] == df[\"first_word_q2\"])\n",
    "    after_first = time.time()\n",
    "    print(\"first word feature loaded for {0} seconds\".format(after_first-after_length))\n",
    "    \n",
    "    # bag of words #\n",
    "#     if tokenizer is None:\n",
    "#         bag_of_word_tokenizer = CountVectorizer(stop_words=\"english\", max_features=top_k_word)\n",
    "#     else:\n",
    "#         bag_of_word_tokenizer = tokenizer\n",
    "#     q1_matrix = bag_of_word_tokenizer.fit_transform(df[\"clean_q1\"]).astype(np.float)\n",
    "#     q2_matrix = bag_of_word_tokenizer.fit_transform(df[\"clean_q2\"]).astype(np.float)\n",
    "#     df[\"vec_q1\"] = [q1_matrix[i] for i in range(len(df))]\n",
    "#     df[\"vec_q2\"] = [q2_matrix[i] for i in range(len(df))]\n",
    "#     print(\"question vectorized\")\n",
    "\n",
    "    \n",
    "    # similarity measure #\n",
    "    #cosine_sim = [cosine_similarity(q1_matrix[i], q2_matrix[i])[0][0] for i in range(len(df))]\n",
    "    #df[\"cosine_sim\"] = cosine_sim\n",
    "    df.loc[:,\"overlap_percent\"] = df.apply(word_overlap, 1)\n",
    "    # Spacy stentence similarity\n",
    "    df.loc[:,\"spacy_sentence_similarity\"] = df.apply(sentence_similarity, 1)\n",
    "    # edit distance\n",
    "    df.loc[:,\"edit_distance\"] = df.apply(lambda x: nltk.edit_distance(x[\"token_1\"], x[\"token_2\"]), 1)\n",
    "    # token Jaccard\n",
    "    df.loc[:,\"token_jaccard\"] = df.apply(lambda x: jaccard_sim(set(x[\"token_1\"]), set(x[\"token_2\"])), 1)\n",
    "    after_sim = time.time()\n",
    "    print(\"similarity feature loaded for {0} seconds\".format(after_sim-after_first))\n",
    "    \n",
    "    # embedding #\n",
    "    # embedding diff -- UGLY\n",
    "    dim_emb = embedding.values()[0].shape[0]\n",
    "    emb_mat = []\n",
    "    df.apply(lambda x: emb_diff(x, embedding, emb_mat), 1)\n",
    "    emb_mat = np.array(emb_mat)\n",
    "    for dim in range(dim_emb):\n",
    "        df[\"emb_diff_dim_{0}\".format(dim)] = emb_mat[:,dim]\n",
    "    # euclidean distance - embedding\n",
    "    df.loc[:,\"emb_dist\"] = df.apply(lambda x: emb_dist(x, embedding), 1)\n",
    "    after_emb = time.time()\n",
    "    print(\"embedding feature loaded for {0} seconds\".format(after_emb-after_sim))\n",
    "    \n",
    "    # entity features #\n",
    "    # entity same\n",
    "    df.loc[:,\"entity_same\"] = df.apply(lambda x: x[\"entity_set_1\"]==x[\"entity_set_2\"], 1)\n",
    "    # entity # same\n",
    "    df.loc[:,\"entity_len_same\"] = df.apply(lambda x: len(x[\"entity_set_1\"])==len(x[\"entity_set_2\"]), 1)\n",
    "    # entity # diff\n",
    "    df.loc[:,\"entity_len_diff\"] = df.apply(lambda x: np.abs(len(x[\"entity_set_1\"])-len(x[\"entity_set_2\"])), 1)\n",
    "    # entity Jaccard\n",
    "    df.loc[:,\"entity_jaccard\"] = df.apply(lambda x: jaccard_sim_unhashbale(x[\"entity_set_1\"], x[\"entity_set_2\"]), 1)\n",
    "    \n",
    "    # noun chunk same\n",
    "    df.loc[:, \"chunk_same\"] = df.apply(lambda x: x[\"noun_chunks_1\"]==x[\"noun_chunks_2\"], 1)\n",
    "    # noun chunk # same\n",
    "    df.loc[:,\"chunk_len_same\"] = df.apply(lambda x: len(x[\"noun_chunks_1\"])==len(x[\"noun_chunks_2\"]), 1)\n",
    "    # noun chunk # diff\n",
    "    df.loc[:,\"chunk_len_diff\"] = df.apply(lambda x: np.abs(len(x[\"noun_chunks_1\"])-len(x[\"noun_chunks_2\"])), 1)\n",
    "    # noun chunk Jaccard\n",
    "    df.loc[:,\"chunk_jaccard\"] = df.apply(lambda x: jaccard_sim_unhashbale(x[\"noun_chunks_1\"], x[\"noun_chunks_2\"]), 1)\n",
    "    after_entity = time.time()\n",
    "    print(\"entity feature loaded for {0} seconds\".format(after_entity-after_emb))\n",
    "    \n",
    "    \n",
    "    # filter columns\n",
    "    ignore_columns = [\"first_word_q1\", \"first_word_q2\", \"clean_q1\", \"clean_q2\", \"token_1\", \"token_2\", \n",
    "                      \"doc1\", \"doc2\", \"cap_doc1\", \"cap_doc2\", \"noun_chunks_1\", \"noun_chunks_2\",\n",
    "                     \"entity_set_1\", \"entity_set_2\"]\n",
    "    col_normalize = ['len_1', 'len_2', 'len_diff', 'edit_distance', 'emb_dist', 'entity_len_diff', 'chunk_len_diff']\n",
    "    #full_feature_df = df\n",
    "    clean_feature_df = df.drop(ignore_columns, axis=1)\n",
    "    if normalize:\n",
    "        for col in clean_feature_df.columns:\n",
    "            if str(col) in col_normalize:\n",
    "                col_max = np.max(clean_feature_df[col])\n",
    "                col_min = np.min(clean_feature_df[col])\n",
    "                clean_feature_df[col] = (clean_feature_df[col]-col_min)/float(col_max-col_min)\n",
    "    after_normalize = time.time()\n",
    "    print(\"normalization time = {0}\".format(time.time()-after_normalize))\n",
    "    print(\"total time = {0}\".format(time.time()-total_begin))\n",
    "    return clean_feature_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DATA CREATION SCRIPTS\n",
    "# Quora Dataset\n",
    "#full_data = pd.read_csv(\"../data/questions.csv\")\n",
    "# Kaggle Dataset\n",
    "# begin_time = time.time()\n",
    "# kaggle_train = pd.read_csv(\"../data/kaggle_competition/origin/train.csv\")\n",
    "# kaggle_test = pd.read_csv(\"../data/kaggle_competition/origin/test.csv\")\n",
    "# print(\"data loaded, used {0} seconds\".format(time.time()-begin_time))\n",
    "\n",
    "# clean dataset\n",
    "# begin_time = time.time()\n",
    "# clean_train = clean_dataset(kaggle_train)\n",
    "# clean_test = clean_dataset(kaggle_test)\n",
    "#clean_train.to_csv(\"../data/kaggle_competition/clean_datasets/clean_train.csv\", index=False)\n",
    "#clean_test.to_csv(\"../data/kaggle_competition/clean_datasets/clean_test.csv\", index=False)\n",
    "#print(\"data cleaned, used {0} seconds\".format(time.time()-begin_time))\n",
    "\n",
    "\n",
    "# split and save dataset\n",
    "#begin_time = time.time()\n",
    "# since Kaggle has its own test set, test_ratio=0\n",
    "# train_set, validation_set, _ = split_dataset(clean_train, 0.8, 0.2, 0)\n",
    "# test_set = clean_test\n",
    "# train_set.to_csv(\"../data/kaggle_competition/clean_kaggle_train.csv\", index=False)\n",
    "# validation_set.to_csv(\"../data/kaggle_competition/clean_kaggle_validation.csv\", index=False)\n",
    "# test_set.to_csv(\"../data/kaggle_competition/clean_kaggle_test.csv\", index=False)\n",
    "# load splitted dataset\n",
    "train_set, validation_set, test_set = load_datasets()\n",
    "# print(\"data splitted, used {0} seconds\".format(time.time()-begin_time))\n",
    "\n",
    "\n",
    "# feature engineering\n",
    "#begin_time = time.time()\n",
    "#feature_train = feature_engineering(train_set)\n",
    "#feature_train.to_csv(\"../data/kaggle_competition/feature_datasets/feature_train_v2.csv\", index=False)\n",
    "#feature_validation = feature_engineering(validation_set)\n",
    "#feature_validation.to_csv(\"../data/kaggle_competition/feature_datasets/feature_validation_v2.csv\", index=False)\n",
    "#feature_test = feature_engineering(test_set)\n",
    "#feature_test.to_csv(\"../data/kaggle_competition/feature_datasets/feature_test_v2.csv\", index=False)\n",
    "#print(\"data featurized, used {0} seconds\".format(time.time()-begin_time))\n",
    "#feature_train, feature_validation, feature_test= load_datasets(load_dir = \"../data/kaggle_competition/feature_datasets\", prefix=\"feature_\", post_fix=\"_v2\")\n",
    "# load splitted dataset\n",
    "# train_set, validation_set, test_set = load_datasets()\n",
    "\n",
    "# split X, y\n",
    "#X_train, y_train = xy_split(feature_train)\n",
    "#X_validate, y_validate = xy_split(feature_validation)\n",
    "#X_test=feature_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323432\n",
      "80858\n",
      "2345796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(validation_set))\n",
    "print(len(test_set))\n",
    "#feature_train = feature_engineering(train_set.head(1000))\n",
    "#length_q1 = test_set.apply(lambda x: len(x[\"clean_q1\"].split()),1)\n",
    "#length_q2 = test_set.apply(lambda x: len(x[\"clean_q2\"].split()),1)\n",
    "#length_q1_train = train_set.apply(lambda x: len(x[\"clean_q1\"].split()),1)\n",
    "#length_q2_train = train_set.apply(lambda x: len(x[\"clean_q1\"].split()),1)\n",
    "#length_q1_valid = validation_set.apply(lambda x: len(x[\"clean_q1\"].split()),1)\n",
    "#length_q2_valid = validation_set.apply(lambda x: len(x[\"clean_q1\"].split()),1)\n",
    "np.max([np.max(length_q1), np.max(length_q2),np.max(length_q1_train),np.max(length_q2_train),\n",
    "        np.max(length_q1_valid),np.max(length_q2_valid)])\n",
    "np.min([np.min(length_q1), np.min(length_q2),np.min(length_q1_train),np.min(length_q2_train),\n",
    "        np.min(length_q1_valid),np.min(length_q2_valid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(323432, 6)\n",
      "(323432,)\n",
      "(80858, 6)\n",
      "(80858,)\n",
      "(2345796, 6)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print y_train.shape\n",
    "print X_validate.shape\n",
    "print y_validate.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65178520641719695"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mat1 = full_feature_df[\"vec_q1\"].iloc[0]\n",
    "#mat2 = full_feature_df[\"vec_q2\"].iloc[0]\n",
    "\n",
    "#cosine_similarity(mat1[5], mat2[5])\n",
    "#a = time.time()\n",
    "#cosine_similarity(full_feature_df[\"vec_q1\"].iloc[10],full_feature_df[\"vec_q2\"].iloc[10] )\n",
    "#print time.time()-a\n",
    "#len(full_feature_df)\n",
    "#full_feature_df.to_csv(\"../data/full_feature_df.csv\", index=False)\n",
    "#np.sum(clean_feature_df[\"cosine_sim\"]==0)/float(len(clean_feature_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mat1 = mat1.astype(np.float)\n",
    "#mat2 = mat2.astype(np.float)\n",
    "#cosine_similarity(mat1, mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PREDICTIVE MODEL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MODEL ANALYTICS FUNCTIONS\n",
    "def all_test_metrics(y_pred, y_test, metrics_list=[\"acc\", \"auc\", \"f1\", \"nll\"]):\n",
    "    score_dict = {}\n",
    "    # acc\n",
    "    if \"acc\" in metrics_list:\n",
    "        y_pred_acc = np.round(y_pred).astype(np.int8)\n",
    "        acc = metrics.accuracy_score(y_test, y_pred_acc, normalize=True)\n",
    "        score_dict[\"acc\"] = acc \n",
    "    # auc\n",
    "    if \"auc\" in metrics_list:\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        score_dict[\"auc\"] = auc\n",
    "        #score_dict[\"fpr\"] = fpr\n",
    "        #score_dict[\"tpr\"] = tpr\n",
    "    # f1-measure\n",
    "    if \"f1\" in metrics_list:\n",
    "        y_pred_acc = np.round(y_pred).astype(np.int8)\n",
    "        f1 = metrics.f1_score(y_test, y_pred_acc, labels=[0,1], pos_label=1)\n",
    "        score_dict[\"f1\"] = f1\n",
    "    # nll\n",
    "    if \"nll\" in metrics_list:\n",
    "        nll = metrics.log_loss(y_test, y_pred)\n",
    "        score_dict[\"nll\"] = nll\n",
    "    return score_dict\n",
    "\n",
    "\n",
    "def test_model(model, X_test, y_test, verbose=True, model_name=\"\", y_pred_test=None, pred_lambda=None):\n",
    "    \"\"\"\n",
    "    Function that generate performance stats for a model\n",
    "    \"\"\"\n",
    "    if y_pred_test is None:\n",
    "        if pred_lambda is None:\n",
    "            y_pred_test = model.predict(X_test)\n",
    "        else:\n",
    "            y_pred_test = pred_lambda(model, X_test)  \n",
    "    scores = all_test_metrics(y_pred_test, y_test)    \n",
    "    if verbose:\n",
    "        print(model_name+\":\")\n",
    "        print(scores)\n",
    "    return y_pred_test, scores\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline 1 - Majority Class (Validation)::\n",
      "{'acc': 0.63063642434885847, 'f1': 0.0, 'auc': 0.5, 'nll': 12.757365947839453}\n",
      "Baseline 2 - Simple Word Overlap (Validation)::\n",
      "{'acc': 0.66991515990996564, 'f1': 0.61955127291387524, 'auc': 0.73065771989901296, 'nll': 0.66146289432171168}\n",
      "Baseline 3 - Simple Logistic Regression (Validation)::\n",
      "{'acc': 0.68349452125949195, 'f1': 0.55166252058442244, 'auc': 0.75714678542401015, 'nll': 0.54561463901481544}\n"
     ]
    }
   ],
   "source": [
    "# MODEL ANALYTICS SCRIPT\n",
    "n_valid = len(validation_set)\n",
    "n_test = len(test_set)\n",
    "\n",
    "# baseline 1: majority class\n",
    "y_pred_valid = [0 for i in range(n_valid)]\n",
    "#y_pred_test = [0 for i in range(n_test)]\n",
    "_, score_majority_class_valid = test_model(None, None, y_validate, \n",
    "                                        verbose=True, model_name=\"Baseline 1 - Majority Class (Validation):\",\n",
    "                                        y_pred_test=y_pred_valid)\n",
    "\n",
    "# baseline 2: simple word overlap\n",
    "y_pred_valid = X_validate[:,5].astype(np.double)\n",
    "#y_pred_test = X_test[:,5].astype(np.double)\n",
    "_, score_majority_class_valid = test_model(None, None, y_validate, \n",
    "                                        verbose=True, model_name=\"Baseline 2 - Simple Word Overlap (Validation):\",\n",
    "                                        y_pred_test=y_pred_valid)\n",
    "\n",
    "# baseline 3: logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr_lambda = lambda model, x: model.predict_proba(x)[:,1]\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred_valid, score_majority_class_valid = test_model(lr, X_validate, y_validate, verbose=True,\n",
    "                                                       model_name=\"Baseline 3 - Simple Logistic Regression (Validation):\",\n",
    "                                                      pred_lambda=lr_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until valid error hasn't decreased in 50 rounds.\n",
      "[0]\ttrain-logloss:0.667396\tvalid-logloss:0.668245\n",
      "[1]\ttrain-logloss:0.647427\tvalid-logloss:0.647754\n",
      "[2]\ttrain-logloss:0.630885\tvalid-logloss:0.630666\n",
      "[3]\ttrain-logloss:0.616394\tvalid-logloss:0.616429\n",
      "[4]\ttrain-logloss:0.604267\tvalid-logloss:0.604557\n",
      "[5]\ttrain-logloss:0.593869\tvalid-logloss:0.594216\n",
      "[6]\ttrain-logloss:0.585130\tvalid-logloss:0.584907\n",
      "[7]\ttrain-logloss:0.577106\tvalid-logloss:0.577148\n",
      "[8]\ttrain-logloss:0.570334\tvalid-logloss:0.570522\n",
      "[9]\ttrain-logloss:0.564192\tvalid-logloss:0.564602\n",
      "[10]\ttrain-logloss:0.558790\tvalid-logloss:0.559089\n",
      "[11]\ttrain-logloss:0.554301\tvalid-logloss:0.554600\n",
      "[12]\ttrain-logloss:0.550374\tvalid-logloss:0.550688\n",
      "[13]\ttrain-logloss:0.546638\tvalid-logloss:0.547129\n",
      "[14]\ttrain-logloss:0.543676\tvalid-logloss:0.544075\n",
      "[15]\ttrain-logloss:0.541156\tvalid-logloss:0.541324\n",
      "[16]\ttrain-logloss:0.538388\tvalid-logloss:0.538522\n",
      "[17]\ttrain-logloss:0.535970\tvalid-logloss:0.536354\n",
      "[18]\ttrain-logloss:0.534203\tvalid-logloss:0.534248\n",
      "[19]\ttrain-logloss:0.532175\tvalid-logloss:0.532499\n",
      "[20]\ttrain-logloss:0.530690\tvalid-logloss:0.530998\n",
      "[21]\ttrain-logloss:0.529286\tvalid-logloss:0.529556\n",
      "[22]\ttrain-logloss:0.527692\tvalid-logloss:0.528267\n",
      "[23]\ttrain-logloss:0.526740\tvalid-logloss:0.526863\n",
      "[24]\ttrain-logloss:0.525670\tvalid-logloss:0.525822\n",
      "[25]\ttrain-logloss:0.524530\tvalid-logloss:0.524845\n",
      "[26]\ttrain-logloss:0.523322\tvalid-logloss:0.523975\n",
      "[27]\ttrain-logloss:0.522545\tvalid-logloss:0.523028\n",
      "[28]\ttrain-logloss:0.522020\tvalid-logloss:0.522206\n",
      "[29]\ttrain-logloss:0.521203\tvalid-logloss:0.521586\n",
      "[30]\ttrain-logloss:0.520242\tvalid-logloss:0.520814\n",
      "[31]\ttrain-logloss:0.519747\tvalid-logloss:0.520098\n",
      "[32]\ttrain-logloss:0.519161\tvalid-logloss:0.519608\n",
      "[33]\ttrain-logloss:0.518719\tvalid-logloss:0.519187\n",
      "[34]\ttrain-logloss:0.518337\tvalid-logloss:0.518822\n",
      "[35]\ttrain-logloss:0.518033\tvalid-logloss:0.518430\n",
      "[36]\ttrain-logloss:0.517748\tvalid-logloss:0.517996\n",
      "[37]\ttrain-logloss:0.516928\tvalid-logloss:0.517595\n",
      "[38]\ttrain-logloss:0.516573\tvalid-logloss:0.517171\n",
      "[39]\ttrain-logloss:0.516251\tvalid-logloss:0.516806\n",
      "[40]\ttrain-logloss:0.515689\tvalid-logloss:0.516190\n",
      "[41]\ttrain-logloss:0.515056\tvalid-logloss:0.515815\n",
      "[42]\ttrain-logloss:0.514832\tvalid-logloss:0.515589\n",
      "[43]\ttrain-logloss:0.514605\tvalid-logloss:0.515333\n",
      "[44]\ttrain-logloss:0.514333\tvalid-logloss:0.515069\n",
      "[45]\ttrain-logloss:0.514137\tvalid-logloss:0.514893\n",
      "[46]\ttrain-logloss:0.513855\tvalid-logloss:0.514627\n",
      "[47]\ttrain-logloss:0.513657\tvalid-logloss:0.514476\n",
      "[48]\ttrain-logloss:0.513367\tvalid-logloss:0.514344\n",
      "[49]\ttrain-logloss:0.513142\tvalid-logloss:0.514169\n",
      "[50]\ttrain-logloss:0.512609\tvalid-logloss:0.513666\n",
      "[51]\ttrain-logloss:0.512250\tvalid-logloss:0.513370\n",
      "[52]\ttrain-logloss:0.511921\tvalid-logloss:0.513169\n",
      "[53]\ttrain-logloss:0.511699\tvalid-logloss:0.512954\n",
      "[54]\ttrain-logloss:0.511381\tvalid-logloss:0.512690\n",
      "[55]\ttrain-logloss:0.511120\tvalid-logloss:0.512452\n",
      "[56]\ttrain-logloss:0.510922\tvalid-logloss:0.512273\n",
      "[57]\ttrain-logloss:0.510723\tvalid-logloss:0.512104\n",
      "[58]\ttrain-logloss:0.510537\tvalid-logloss:0.511974\n",
      "[59]\ttrain-logloss:0.510369\tvalid-logloss:0.511833\n",
      "[60]\ttrain-logloss:0.510102\tvalid-logloss:0.511548\n",
      "[61]\ttrain-logloss:0.509874\tvalid-logloss:0.511370\n",
      "[62]\ttrain-logloss:0.509569\tvalid-logloss:0.511090\n",
      "[63]\ttrain-logloss:0.509284\tvalid-logloss:0.510866\n",
      "[64]\ttrain-logloss:0.509125\tvalid-logloss:0.510748\n",
      "[65]\ttrain-logloss:0.508837\tvalid-logloss:0.510483\n",
      "[66]\ttrain-logloss:0.508601\tvalid-logloss:0.510258\n",
      "[67]\ttrain-logloss:0.508360\tvalid-logloss:0.510028\n",
      "[68]\ttrain-logloss:0.508202\tvalid-logloss:0.509921\n",
      "[69]\ttrain-logloss:0.508059\tvalid-logloss:0.509811\n",
      "[70]\ttrain-logloss:0.507883\tvalid-logloss:0.509679\n",
      "[71]\ttrain-logloss:0.507733\tvalid-logloss:0.509531\n",
      "[72]\ttrain-logloss:0.507498\tvalid-logloss:0.509323\n",
      "[73]\ttrain-logloss:0.507015\tvalid-logloss:0.508841\n",
      "[74]\ttrain-logloss:0.506811\tvalid-logloss:0.508696\n",
      "[75]\ttrain-logloss:0.506629\tvalid-logloss:0.508544\n",
      "[76]\ttrain-logloss:0.506405\tvalid-logloss:0.508382\n",
      "[77]\ttrain-logloss:0.506289\tvalid-logloss:0.508279\n",
      "[78]\ttrain-logloss:0.505947\tvalid-logloss:0.507950\n",
      "[79]\ttrain-logloss:0.505769\tvalid-logloss:0.507798\n",
      "[80]\ttrain-logloss:0.505641\tvalid-logloss:0.507702\n",
      "[81]\ttrain-logloss:0.505469\tvalid-logloss:0.507586\n",
      "[82]\ttrain-logloss:0.505267\tvalid-logloss:0.507446\n",
      "[83]\ttrain-logloss:0.505086\tvalid-logloss:0.507318\n",
      "[84]\ttrain-logloss:0.504937\tvalid-logloss:0.507195\n",
      "[85]\ttrain-logloss:0.504728\tvalid-logloss:0.507002\n",
      "[86]\ttrain-logloss:0.504573\tvalid-logloss:0.506867\n",
      "[87]\ttrain-logloss:0.504440\tvalid-logloss:0.506783\n",
      "[88]\ttrain-logloss:0.504339\tvalid-logloss:0.506681\n",
      "[89]\ttrain-logloss:0.504222\tvalid-logloss:0.506584\n",
      "[90]\ttrain-logloss:0.503933\tvalid-logloss:0.506291\n",
      "[91]\ttrain-logloss:0.503807\tvalid-logloss:0.506191\n",
      "[92]\ttrain-logloss:0.503543\tvalid-logloss:0.505979\n",
      "[93]\ttrain-logloss:0.503435\tvalid-logloss:0.505932\n",
      "[94]\ttrain-logloss:0.503301\tvalid-logloss:0.505855\n",
      "[95]\ttrain-logloss:0.503192\tvalid-logloss:0.505756\n",
      "[96]\ttrain-logloss:0.502986\tvalid-logloss:0.505573\n",
      "[97]\ttrain-logloss:0.502869\tvalid-logloss:0.505508\n",
      "[98]\ttrain-logloss:0.502351\tvalid-logloss:0.505022\n",
      "[99]\ttrain-logloss:0.502125\tvalid-logloss:0.504825\n",
      "[100]\ttrain-logloss:0.501991\tvalid-logloss:0.504732\n",
      "[101]\ttrain-logloss:0.501808\tvalid-logloss:0.504604\n",
      "[102]\ttrain-logloss:0.501344\tvalid-logloss:0.504130\n",
      "[103]\ttrain-logloss:0.501206\tvalid-logloss:0.504041\n",
      "[104]\ttrain-logloss:0.501119\tvalid-logloss:0.503987\n",
      "[105]\ttrain-logloss:0.500987\tvalid-logloss:0.503886\n",
      "[106]\ttrain-logloss:0.500806\tvalid-logloss:0.503705\n",
      "[107]\ttrain-logloss:0.500700\tvalid-logloss:0.503647\n",
      "[108]\ttrain-logloss:0.500543\tvalid-logloss:0.503519\n",
      "[109]\ttrain-logloss:0.500453\tvalid-logloss:0.503450\n",
      "[110]\ttrain-logloss:0.500351\tvalid-logloss:0.503379\n",
      "[111]\ttrain-logloss:0.500210\tvalid-logloss:0.503264\n",
      "[112]\ttrain-logloss:0.500103\tvalid-logloss:0.503199\n",
      "[113]\ttrain-logloss:0.500004\tvalid-logloss:0.503109\n",
      "[114]\ttrain-logloss:0.499898\tvalid-logloss:0.503071\n",
      "[115]\ttrain-logloss:0.499804\tvalid-logloss:0.503014\n",
      "[116]\ttrain-logloss:0.499716\tvalid-logloss:0.502941\n",
      "[117]\ttrain-logloss:0.499613\tvalid-logloss:0.502886\n",
      "[118]\ttrain-logloss:0.499542\tvalid-logloss:0.502823\n",
      "[119]\ttrain-logloss:0.499403\tvalid-logloss:0.502706\n",
      "[120]\ttrain-logloss:0.499326\tvalid-logloss:0.502681\n",
      "[121]\ttrain-logloss:0.499187\tvalid-logloss:0.502559\n",
      "[122]\ttrain-logloss:0.498959\tvalid-logloss:0.502377\n",
      "[123]\ttrain-logloss:0.498852\tvalid-logloss:0.502298\n",
      "[124]\ttrain-logloss:0.498659\tvalid-logloss:0.502115\n",
      "[125]\ttrain-logloss:0.498436\tvalid-logloss:0.501931\n",
      "[126]\ttrain-logloss:0.498316\tvalid-logloss:0.501861\n",
      "[127]\ttrain-logloss:0.497976\tvalid-logloss:0.501511\n",
      "[128]\ttrain-logloss:0.497900\tvalid-logloss:0.501448\n",
      "[129]\ttrain-logloss:0.497830\tvalid-logloss:0.501415\n",
      "[130]\ttrain-logloss:0.497788\tvalid-logloss:0.501392\n",
      "[131]\ttrain-logloss:0.497656\tvalid-logloss:0.501300\n",
      "[132]\ttrain-logloss:0.497584\tvalid-logloss:0.501259\n",
      "[133]\ttrain-logloss:0.497496\tvalid-logloss:0.501190\n",
      "[134]\ttrain-logloss:0.497317\tvalid-logloss:0.501057\n",
      "[135]\ttrain-logloss:0.497148\tvalid-logloss:0.500924\n",
      "[136]\ttrain-logloss:0.497064\tvalid-logloss:0.500877\n",
      "[137]\ttrain-logloss:0.497014\tvalid-logloss:0.500851\n",
      "[138]\ttrain-logloss:0.496934\tvalid-logloss:0.500796\n",
      "[139]\ttrain-logloss:0.496868\tvalid-logloss:0.500786\n",
      "[140]\ttrain-logloss:0.496801\tvalid-logloss:0.500751\n",
      "[141]\ttrain-logloss:0.496751\tvalid-logloss:0.500722\n",
      "[142]\ttrain-logloss:0.496653\tvalid-logloss:0.500662\n",
      "[143]\ttrain-logloss:0.496557\tvalid-logloss:0.500615\n",
      "[144]\ttrain-logloss:0.496453\tvalid-logloss:0.500559\n",
      "[145]\ttrain-logloss:0.496392\tvalid-logloss:0.500496\n",
      "[146]\ttrain-logloss:0.496305\tvalid-logloss:0.500462\n",
      "[147]\ttrain-logloss:0.496199\tvalid-logloss:0.500393\n",
      "[148]\ttrain-logloss:0.495852\tvalid-logloss:0.500038\n",
      "[149]\ttrain-logloss:0.495779\tvalid-logloss:0.499985\n",
      "[150]\ttrain-logloss:0.495595\tvalid-logloss:0.499814\n",
      "[151]\ttrain-logloss:0.495524\tvalid-logloss:0.499778\n",
      "[152]\ttrain-logloss:0.495440\tvalid-logloss:0.499739\n",
      "[153]\ttrain-logloss:0.495361\tvalid-logloss:0.499707\n",
      "[154]\ttrain-logloss:0.495023\tvalid-logloss:0.499398\n",
      "[155]\ttrain-logloss:0.494936\tvalid-logloss:0.499334\n",
      "[156]\ttrain-logloss:0.494899\tvalid-logloss:0.499309\n",
      "[157]\ttrain-logloss:0.494856\tvalid-logloss:0.499282\n",
      "[158]\ttrain-logloss:0.494803\tvalid-logloss:0.499263\n",
      "[159]\ttrain-logloss:0.494643\tvalid-logloss:0.499142\n",
      "[160]\ttrain-logloss:0.494576\tvalid-logloss:0.499109\n",
      "[161]\ttrain-logloss:0.494539\tvalid-logloss:0.499091\n",
      "[162]\ttrain-logloss:0.494472\tvalid-logloss:0.499060\n",
      "[163]\ttrain-logloss:0.494440\tvalid-logloss:0.499022\n",
      "[164]\ttrain-logloss:0.494382\tvalid-logloss:0.498998\n",
      "[165]\ttrain-logloss:0.494293\tvalid-logloss:0.498935\n",
      "[166]\ttrain-logloss:0.494196\tvalid-logloss:0.498853\n",
      "[167]\ttrain-logloss:0.494073\tvalid-logloss:0.498774\n",
      "[168]\ttrain-logloss:0.494032\tvalid-logloss:0.498766\n",
      "[169]\ttrain-logloss:0.493951\tvalid-logloss:0.498734\n",
      "[170]\ttrain-logloss:0.493873\tvalid-logloss:0.498674\n",
      "[171]\ttrain-logloss:0.493801\tvalid-logloss:0.498644\n",
      "[172]\ttrain-logloss:0.493766\tvalid-logloss:0.498627\n",
      "[173]\ttrain-logloss:0.493693\tvalid-logloss:0.498559\n",
      "[174]\ttrain-logloss:0.493601\tvalid-logloss:0.498516\n",
      "[175]\ttrain-logloss:0.493494\tvalid-logloss:0.498439\n",
      "[176]\ttrain-logloss:0.493436\tvalid-logloss:0.498393\n",
      "[177]\ttrain-logloss:0.493351\tvalid-logloss:0.498341\n",
      "[178]\ttrain-logloss:0.493313\tvalid-logloss:0.498333\n",
      "[179]\ttrain-logloss:0.493211\tvalid-logloss:0.498284\n",
      "[180]\ttrain-logloss:0.493136\tvalid-logloss:0.498267\n",
      "[181]\ttrain-logloss:0.493069\tvalid-logloss:0.498234\n",
      "[182]\ttrain-logloss:0.493018\tvalid-logloss:0.498204\n",
      "[183]\ttrain-logloss:0.492933\tvalid-logloss:0.498147\n",
      "[184]\ttrain-logloss:0.492877\tvalid-logloss:0.498116\n",
      "[185]\ttrain-logloss:0.492838\tvalid-logloss:0.498106\n",
      "[186]\ttrain-logloss:0.492787\tvalid-logloss:0.498093\n",
      "[187]\ttrain-logloss:0.492673\tvalid-logloss:0.497988\n",
      "[188]\ttrain-logloss:0.492610\tvalid-logloss:0.497955\n",
      "[189]\ttrain-logloss:0.492580\tvalid-logloss:0.497956\n",
      "[190]\ttrain-logloss:0.492449\tvalid-logloss:0.497844\n",
      "[191]\ttrain-logloss:0.492356\tvalid-logloss:0.497774\n",
      "[192]\ttrain-logloss:0.492013\tvalid-logloss:0.497449\n",
      "[193]\ttrain-logloss:0.491852\tvalid-logloss:0.497319\n",
      "[194]\ttrain-logloss:0.491783\tvalid-logloss:0.497281\n",
      "[195]\ttrain-logloss:0.491710\tvalid-logloss:0.497244\n",
      "[196]\ttrain-logloss:0.491684\tvalid-logloss:0.497235\n",
      "[197]\ttrain-logloss:0.491615\tvalid-logloss:0.497208\n",
      "[198]\ttrain-logloss:0.491555\tvalid-logloss:0.497197\n",
      "[199]\ttrain-logloss:0.491490\tvalid-logloss:0.497152\n",
      "[200]\ttrain-logloss:0.491453\tvalid-logloss:0.497134\n",
      "[201]\ttrain-logloss:0.491340\tvalid-logloss:0.497045\n",
      "[202]\ttrain-logloss:0.491302\tvalid-logloss:0.497034\n",
      "[203]\ttrain-logloss:0.491224\tvalid-logloss:0.496972\n",
      "[204]\ttrain-logloss:0.491037\tvalid-logloss:0.496815\n",
      "[205]\ttrain-logloss:0.490982\tvalid-logloss:0.496780\n",
      "[206]\ttrain-logloss:0.490934\tvalid-logloss:0.496739\n",
      "[207]\ttrain-logloss:0.490876\tvalid-logloss:0.496687\n",
      "[208]\ttrain-logloss:0.490701\tvalid-logloss:0.496547\n",
      "[209]\ttrain-logloss:0.490622\tvalid-logloss:0.496504\n",
      "[210]\ttrain-logloss:0.490604\tvalid-logloss:0.496504\n",
      "[211]\ttrain-logloss:0.490541\tvalid-logloss:0.496450\n",
      "[212]\ttrain-logloss:0.490485\tvalid-logloss:0.496419\n",
      "[213]\ttrain-logloss:0.490436\tvalid-logloss:0.496411\n",
      "[214]\ttrain-logloss:0.490412\tvalid-logloss:0.496404\n",
      "[215]\ttrain-logloss:0.490389\tvalid-logloss:0.496403\n",
      "[216]\ttrain-logloss:0.490337\tvalid-logloss:0.496377\n",
      "[217]\ttrain-logloss:0.490277\tvalid-logloss:0.496353\n",
      "[218]\ttrain-logloss:0.490203\tvalid-logloss:0.496316\n",
      "[219]\ttrain-logloss:0.490164\tvalid-logloss:0.496318\n",
      "[220]\ttrain-logloss:0.490003\tvalid-logloss:0.496192\n",
      "[221]\ttrain-logloss:0.489755\tvalid-logloss:0.495962\n",
      "[222]\ttrain-logloss:0.489657\tvalid-logloss:0.495898\n",
      "[223]\ttrain-logloss:0.489606\tvalid-logloss:0.495885\n",
      "[224]\ttrain-logloss:0.489558\tvalid-logloss:0.495857\n",
      "[225]\ttrain-logloss:0.489485\tvalid-logloss:0.495811\n",
      "[226]\ttrain-logloss:0.489460\tvalid-logloss:0.495809\n",
      "[227]\ttrain-logloss:0.489419\tvalid-logloss:0.495797\n",
      "[228]\ttrain-logloss:0.489371\tvalid-logloss:0.495775\n",
      "[229]\ttrain-logloss:0.489339\tvalid-logloss:0.495768\n",
      "[230]\ttrain-logloss:0.489295\tvalid-logloss:0.495735\n",
      "[231]\ttrain-logloss:0.489243\tvalid-logloss:0.495712\n",
      "[232]\ttrain-logloss:0.489126\tvalid-logloss:0.495642\n",
      "[233]\ttrain-logloss:0.488983\tvalid-logloss:0.495526\n",
      "[234]\ttrain-logloss:0.488782\tvalid-logloss:0.495359\n",
      "[235]\ttrain-logloss:0.488727\tvalid-logloss:0.495340\n",
      "[236]\ttrain-logloss:0.488666\tvalid-logloss:0.495321\n",
      "[237]\ttrain-logloss:0.488617\tvalid-logloss:0.495291\n",
      "[238]\ttrain-logloss:0.488598\tvalid-logloss:0.495277\n",
      "[239]\ttrain-logloss:0.488574\tvalid-logloss:0.495265\n",
      "[240]\ttrain-logloss:0.488483\tvalid-logloss:0.495206\n",
      "[241]\ttrain-logloss:0.488430\tvalid-logloss:0.495175\n",
      "[242]\ttrain-logloss:0.488358\tvalid-logloss:0.495164\n",
      "[243]\ttrain-logloss:0.488244\tvalid-logloss:0.495077\n",
      "[244]\ttrain-logloss:0.488047\tvalid-logloss:0.494881\n",
      "[245]\ttrain-logloss:0.487991\tvalid-logloss:0.494856\n",
      "[246]\ttrain-logloss:0.487960\tvalid-logloss:0.494848\n",
      "[247]\ttrain-logloss:0.487903\tvalid-logloss:0.494832\n",
      "[248]\ttrain-logloss:0.487798\tvalid-logloss:0.494787\n",
      "[249]\ttrain-logloss:0.487714\tvalid-logloss:0.494736\n",
      "[250]\ttrain-logloss:0.487644\tvalid-logloss:0.494705\n",
      "[251]\ttrain-logloss:0.487562\tvalid-logloss:0.494655\n",
      "[252]\ttrain-logloss:0.487541\tvalid-logloss:0.494656\n",
      "[253]\ttrain-logloss:0.487516\tvalid-logloss:0.494650\n",
      "[254]\ttrain-logloss:0.487501\tvalid-logloss:0.494648\n",
      "[255]\ttrain-logloss:0.487393\tvalid-logloss:0.494542\n",
      "[256]\ttrain-logloss:0.487320\tvalid-logloss:0.494489\n",
      "[257]\ttrain-logloss:0.487301\tvalid-logloss:0.494482\n",
      "[258]\ttrain-logloss:0.487271\tvalid-logloss:0.494485\n",
      "[259]\ttrain-logloss:0.487246\tvalid-logloss:0.494482\n",
      "[260]\ttrain-logloss:0.487043\tvalid-logloss:0.494331\n",
      "[261]\ttrain-logloss:0.486870\tvalid-logloss:0.494180\n",
      "[262]\ttrain-logloss:0.486602\tvalid-logloss:0.493956\n",
      "[263]\ttrain-logloss:0.486534\tvalid-logloss:0.493924\n",
      "[264]\ttrain-logloss:0.486482\tvalid-logloss:0.493919\n",
      "[265]\ttrain-logloss:0.486386\tvalid-logloss:0.493849\n",
      "[266]\ttrain-logloss:0.486359\tvalid-logloss:0.493835\n",
      "[267]\ttrain-logloss:0.486239\tvalid-logloss:0.493760\n",
      "[268]\ttrain-logloss:0.486194\tvalid-logloss:0.493744\n",
      "[269]\ttrain-logloss:0.486149\tvalid-logloss:0.493732\n",
      "[270]\ttrain-logloss:0.486083\tvalid-logloss:0.493697\n",
      "[271]\ttrain-logloss:0.485994\tvalid-logloss:0.493640\n",
      "[272]\ttrain-logloss:0.485978\tvalid-logloss:0.493641\n",
      "[273]\ttrain-logloss:0.485969\tvalid-logloss:0.493636\n",
      "[274]\ttrain-logloss:0.485957\tvalid-logloss:0.493636\n",
      "[275]\ttrain-logloss:0.485847\tvalid-logloss:0.493579\n",
      "[276]\ttrain-logloss:0.485791\tvalid-logloss:0.493568\n",
      "[277]\ttrain-logloss:0.485764\tvalid-logloss:0.493564\n",
      "[278]\ttrain-logloss:0.485632\tvalid-logloss:0.493464\n",
      "[279]\ttrain-logloss:0.485564\tvalid-logloss:0.493433\n",
      "[280]\ttrain-logloss:0.485524\tvalid-logloss:0.493439\n",
      "[281]\ttrain-logloss:0.485502\tvalid-logloss:0.493443\n",
      "[282]\ttrain-logloss:0.485468\tvalid-logloss:0.493429\n",
      "[283]\ttrain-logloss:0.485426\tvalid-logloss:0.493420\n",
      "[284]\ttrain-logloss:0.485374\tvalid-logloss:0.493392\n",
      "[285]\ttrain-logloss:0.485274\tvalid-logloss:0.493331\n",
      "[286]\ttrain-logloss:0.485255\tvalid-logloss:0.493324\n",
      "[287]\ttrain-logloss:0.485235\tvalid-logloss:0.493326\n",
      "[288]\ttrain-logloss:0.485146\tvalid-logloss:0.493301\n",
      "[289]\ttrain-logloss:0.485015\tvalid-logloss:0.493204\n",
      "[290]\ttrain-logloss:0.484904\tvalid-logloss:0.493141\n",
      "[291]\ttrain-logloss:0.484752\tvalid-logloss:0.493033\n",
      "[292]\ttrain-logloss:0.484706\tvalid-logloss:0.493019\n",
      "[293]\ttrain-logloss:0.484561\tvalid-logloss:0.492922\n",
      "[294]\ttrain-logloss:0.484518\tvalid-logloss:0.492915\n",
      "[295]\ttrain-logloss:0.484468\tvalid-logloss:0.492904\n",
      "[296]\ttrain-logloss:0.484447\tvalid-logloss:0.492902\n",
      "[297]\ttrain-logloss:0.484330\tvalid-logloss:0.492825\n",
      "[298]\ttrain-logloss:0.484253\tvalid-logloss:0.492778\n",
      "[299]\ttrain-logloss:0.484224\tvalid-logloss:0.492754\n",
      "[300]\ttrain-logloss:0.484084\tvalid-logloss:0.492661\n",
      "[301]\ttrain-logloss:0.484016\tvalid-logloss:0.492636\n",
      "[302]\ttrain-logloss:0.483999\tvalid-logloss:0.492630\n",
      "[303]\ttrain-logloss:0.483975\tvalid-logloss:0.492625\n",
      "[304]\ttrain-logloss:0.483963\tvalid-logloss:0.492627\n",
      "[305]\ttrain-logloss:0.483950\tvalid-logloss:0.492627\n",
      "[306]\ttrain-logloss:0.483900\tvalid-logloss:0.492616\n",
      "[307]\ttrain-logloss:0.483855\tvalid-logloss:0.492596\n",
      "[308]\ttrain-logloss:0.483773\tvalid-logloss:0.492556\n",
      "[309]\ttrain-logloss:0.483698\tvalid-logloss:0.492497\n",
      "[310]\ttrain-logloss:0.483686\tvalid-logloss:0.492507\n",
      "[311]\ttrain-logloss:0.483628\tvalid-logloss:0.492501\n",
      "[312]\ttrain-logloss:0.483581\tvalid-logloss:0.492490\n",
      "[313]\ttrain-logloss:0.483538\tvalid-logloss:0.492473\n",
      "[314]\ttrain-logloss:0.483481\tvalid-logloss:0.492454\n",
      "[315]\ttrain-logloss:0.483460\tvalid-logloss:0.492449\n",
      "[316]\ttrain-logloss:0.483327\tvalid-logloss:0.492340\n",
      "[317]\ttrain-logloss:0.483147\tvalid-logloss:0.492197\n",
      "[318]\ttrain-logloss:0.483027\tvalid-logloss:0.492093\n",
      "[319]\ttrain-logloss:0.482925\tvalid-logloss:0.492017\n",
      "[320]\ttrain-logloss:0.482915\tvalid-logloss:0.492019\n",
      "[321]\ttrain-logloss:0.482861\tvalid-logloss:0.492013\n",
      "[322]\ttrain-logloss:0.482843\tvalid-logloss:0.492009\n",
      "[323]\ttrain-logloss:0.482735\tvalid-logloss:0.491952\n",
      "[324]\ttrain-logloss:0.482665\tvalid-logloss:0.491913\n",
      "[325]\ttrain-logloss:0.482625\tvalid-logloss:0.491909\n",
      "[326]\ttrain-logloss:0.482617\tvalid-logloss:0.491906\n",
      "[327]\ttrain-logloss:0.482594\tvalid-logloss:0.491903\n",
      "[328]\ttrain-logloss:0.482578\tvalid-logloss:0.491900\n",
      "[329]\ttrain-logloss:0.482519\tvalid-logloss:0.491884\n",
      "[330]\ttrain-logloss:0.482481\tvalid-logloss:0.491876\n",
      "[331]\ttrain-logloss:0.482457\tvalid-logloss:0.491869\n",
      "[332]\ttrain-logloss:0.482419\tvalid-logloss:0.491881\n",
      "[333]\ttrain-logloss:0.482294\tvalid-logloss:0.491799\n",
      "[334]\ttrain-logloss:0.482205\tvalid-logloss:0.491734\n",
      "[335]\ttrain-logloss:0.482040\tvalid-logloss:0.491579\n",
      "[336]\ttrain-logloss:0.481952\tvalid-logloss:0.491550\n",
      "[337]\ttrain-logloss:0.481831\tvalid-logloss:0.491456\n",
      "[338]\ttrain-logloss:0.481773\tvalid-logloss:0.491439\n",
      "[339]\ttrain-logloss:0.481714\tvalid-logloss:0.491417\n",
      "[340]\ttrain-logloss:0.481590\tvalid-logloss:0.491334\n",
      "[341]\ttrain-logloss:0.481507\tvalid-logloss:0.491276\n",
      "[342]\ttrain-logloss:0.481481\tvalid-logloss:0.491269\n",
      "[343]\ttrain-logloss:0.481429\tvalid-logloss:0.491236\n",
      "[344]\ttrain-logloss:0.481382\tvalid-logloss:0.491215\n",
      "[345]\ttrain-logloss:0.481327\tvalid-logloss:0.491198\n",
      "[346]\ttrain-logloss:0.481277\tvalid-logloss:0.491194\n",
      "[347]\ttrain-logloss:0.481254\tvalid-logloss:0.491193\n",
      "[348]\ttrain-logloss:0.481231\tvalid-logloss:0.491178\n",
      "[349]\ttrain-logloss:0.481151\tvalid-logloss:0.491131\n",
      "[350]\ttrain-logloss:0.481110\tvalid-logloss:0.491109\n",
      "[351]\ttrain-logloss:0.481102\tvalid-logloss:0.491115\n",
      "[352]\ttrain-logloss:0.481081\tvalid-logloss:0.491121\n",
      "[353]\ttrain-logloss:0.481070\tvalid-logloss:0.491119\n",
      "[354]\ttrain-logloss:0.481046\tvalid-logloss:0.491114\n",
      "[355]\ttrain-logloss:0.481028\tvalid-logloss:0.491119\n",
      "[356]\ttrain-logloss:0.481020\tvalid-logloss:0.491121\n",
      "[357]\ttrain-logloss:0.480992\tvalid-logloss:0.491116\n",
      "[358]\ttrain-logloss:0.480919\tvalid-logloss:0.491073\n",
      "[359]\ttrain-logloss:0.480866\tvalid-logloss:0.491059\n",
      "[360]\ttrain-logloss:0.480824\tvalid-logloss:0.491053\n",
      "[361]\ttrain-logloss:0.480783\tvalid-logloss:0.491034\n",
      "[362]\ttrain-logloss:0.480727\tvalid-logloss:0.491003\n",
      "[363]\ttrain-logloss:0.480679\tvalid-logloss:0.490994\n",
      "[364]\ttrain-logloss:0.480611\tvalid-logloss:0.490940\n",
      "[365]\ttrain-logloss:0.480585\tvalid-logloss:0.490938\n",
      "[366]\ttrain-logloss:0.480537\tvalid-logloss:0.490917\n",
      "[367]\ttrain-logloss:0.480520\tvalid-logloss:0.490919\n",
      "[368]\ttrain-logloss:0.480359\tvalid-logloss:0.490763\n",
      "[369]\ttrain-logloss:0.480325\tvalid-logloss:0.490767\n",
      "[370]\ttrain-logloss:0.480305\tvalid-logloss:0.490768\n",
      "[371]\ttrain-logloss:0.480298\tvalid-logloss:0.490767\n",
      "[372]\ttrain-logloss:0.480262\tvalid-logloss:0.490760\n",
      "[373]\ttrain-logloss:0.480257\tvalid-logloss:0.490758\n",
      "[374]\ttrain-logloss:0.480245\tvalid-logloss:0.490756\n",
      "[375]\ttrain-logloss:0.480225\tvalid-logloss:0.490757\n",
      "[376]\ttrain-logloss:0.480170\tvalid-logloss:0.490726\n",
      "[377]\ttrain-logloss:0.480038\tvalid-logloss:0.490617\n",
      "[378]\ttrain-logloss:0.479916\tvalid-logloss:0.490527\n",
      "[379]\ttrain-logloss:0.479762\tvalid-logloss:0.490426\n",
      "[380]\ttrain-logloss:0.479647\tvalid-logloss:0.490334\n",
      "[381]\ttrain-logloss:0.479583\tvalid-logloss:0.490293\n",
      "[382]\ttrain-logloss:0.479504\tvalid-logloss:0.490242\n",
      "[383]\ttrain-logloss:0.479303\tvalid-logloss:0.490064\n",
      "[384]\ttrain-logloss:0.479218\tvalid-logloss:0.490040\n",
      "[385]\ttrain-logloss:0.479138\tvalid-logloss:0.489994\n",
      "[386]\ttrain-logloss:0.479016\tvalid-logloss:0.489926\n",
      "[387]\ttrain-logloss:0.478842\tvalid-logloss:0.489789\n",
      "[388]\ttrain-logloss:0.478752\tvalid-logloss:0.489729\n",
      "[389]\ttrain-logloss:0.478684\tvalid-logloss:0.489675\n",
      "[390]\ttrain-logloss:0.478608\tvalid-logloss:0.489646\n",
      "[391]\ttrain-logloss:0.478575\tvalid-logloss:0.489639\n",
      "[392]\ttrain-logloss:0.478524\tvalid-logloss:0.489638\n",
      "[393]\ttrain-logloss:0.478438\tvalid-logloss:0.489580\n",
      "[394]\ttrain-logloss:0.478366\tvalid-logloss:0.489554\n",
      "[395]\ttrain-logloss:0.478291\tvalid-logloss:0.489526\n",
      "[396]\ttrain-logloss:0.478130\tvalid-logloss:0.489391\n",
      "[397]\ttrain-logloss:0.478033\tvalid-logloss:0.489303\n",
      "[398]\ttrain-logloss:0.477953\tvalid-logloss:0.489272\n",
      "[399]\ttrain-logloss:0.477830\tvalid-logloss:0.489170\n"
     ]
    }
   ],
   "source": [
    "#XG BOOST\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.1\n",
    "params['max_depth'] = 4\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(X_validate, label=y_validate)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add single entity feature \n",
    "def upper_str(input_str):\n",
    "    input_str = input_str.upper()\n",
    "    return input_str\n",
    "\n",
    "def upper_dataset(full_dataset):\n",
    "    \"\"\"\n",
    "    Function that cleans the full dataset\n",
    "    \"\"\"\n",
    "    full_dataset[\"clean_q1\"] = full_dataset[\"clean_q1\"].apply(upper_str,1)\n",
    "    full_dataset[\"clean_q2\"] = full_dataset[\"clean_q2\"].apply(upper_str,1)\n",
    "    return full_dataset\n",
    "\n",
    "X_train=upper_dataset(X_train)\n",
    "X_valid=upper_dataset(X_validate)\n",
    "X_test=upper_dataset(X_test)\n",
    "\n",
    "def singleentity(row):\n",
    "    \"\"\"\n",
    "    Function that calculates where there is any entity difference between the two questions\n",
    "    \"\"\"\n",
    "    doc_1 = nlp(row[\"clean_q1\"])\n",
    "    doc_2 = nlp(row[\"clean_q2\"])\n",
    "    if len(doc_1.ents)!=len(doc_2.ents):\n",
    "        return 1\n",
    "    else:\n",
    "        for ent in doc_2.ents:\n",
    "            if ent not in doc_1.ents:\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "X_train[\"singleentity\"]=X_train.apply(singleentity,1)\n",
    "X_valid[\"singleentity\"]=X_valid.apply(singleentity,1)\n",
    "X_test[\"singleentity\"]=X_test.apply(singleentity,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add edit distance feature\n",
    "def distance(row):\n",
    "    \"\"\"\n",
    "    Function that calculates the percentage of edit distance over the average length\n",
    "    \"\"\"\n",
    "    token_1 = nltk.word_tokenize(row[\"clean_q1\"])\n",
    "    token_2 = nltk.word_tokenize(row[\"clean_q2\"])\n",
    "    avg_length = float(len(token_1)+len(token_2))/2\n",
    "    return float(nltk.edit_distance(token_1,token_2))/avg_length\n",
    "\n",
    "X_train[\"Edit_distance\"]=X_train.apply(distance,1)\n",
    "X_valid[\"Edit_distance\"]=X_valid.apply(distance,1)\n",
    "X_test[\"Edit_distance\"]=X_test.apply(distance,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add % of length of the longest common sequence\n",
    "#### May not use as it takes a long time to run ####\n",
    "def lcs(xstr, ystr):\n",
    "    if not xstr or not ystr:\n",
    "        return 0\n",
    "    x, xs, y, ys = xstr[0], xstr[1:], ystr[0], ystr[1:]\n",
    "    if x == y:\n",
    "        return 1 + lcs(xs, ys)\n",
    "    else:\n",
    "        return max(lcs(xstr, ys), lcs(xs, ystr))\n",
    "\n",
    "def longestcommonsequence(row):\n",
    "    token_1 = nltk.word_tokenize(row[\"clean_q1\"])\n",
    "    token_2 = nltk.word_tokenize(row[\"clean_q2\"])\n",
    "    avg_length = float(len(token_1)+len(token_2))/2\n",
    "    return lcs(token_1,token_2)/avg_length  \n",
    "\n",
    "X_train[\"LCS\"]=X_train.apply(longestcommonsequence,1)\n",
    "X_valid[\"LCS\"]=X_valid.apply(longestcommonsequence,1)\n",
    "X_test[\"LCS\"]=X_test.apply(longestcommonsequence,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add similarity score \n",
    "def similarity(row):\n",
    "    sent_1 = nlp(row[\"clean_q1\"])\n",
    "    sent_2 = nlp(row[\"clean_q2\"])\n",
    "    return sent_1.similarity(sent_2)  \n",
    "\n",
    "X_train[\"Similarity\"]=X_train.apply(similarity,1)\n",
    "X_valid[\"Similarity\"]=X_valid.apply(similarity,1)\n",
    "X_test[\"Similarity\"]=X_test.apply(similarity,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#XG BOOST\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 4\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(X_validate, label=y_validate)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 2000, watchlist, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert format to support BiMPM\n",
    "def convert_dataset_to_BiMPM(df, name, save_dir=\"../../BiMPM/data\", test=False):\n",
    "    if test:\n",
    "        df[\"is_duplicate\"] = \"-\"\n",
    "    #df[\"non_empty_len_1\"] = df.apply(lambda x: len(x[\"clean_q1\"].replace(\" \",\"\")),1)\n",
    "    #df[\"non_empty_len_2\"] = df.apply(lambda x: len(x[\"clean_q2\"].replace(\" \",\"\")),1)\n",
    "    #df[\"retain\"] = df.apply(lambda x: x[\"non_empty_len_1\"]>0 and x[\"non_empty_len_2\"]>0,1)\n",
    "    wrote_order = [\"is_duplicate\", \"clean_q1\", \"clean_q2\"]\n",
    "    df.to_csv(os.path.join(save_dir, \"bimpm_{0}.csv\".format(name)), columns=wrote_order, sep=\"\\t\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#validation_set.head()\n",
    "convert_dataset_to_BiMPM(validation_set, \"validation\")\n",
    "#validation_set[~validation_set[\"retain\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir ../../BiMPM/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is kaley cuoco doing her best in season 8? any comments on her new look?'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set.iloc[8658][\"clean_q2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_q1</th>\n",
       "      <th>clean_q2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how can i increase height after 22?</td>\n",
       "      <td>what is the way to increase the height at the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what do you do to get an adrenaline rush?</td>\n",
       "      <td>what is an adrenaline rush?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>which phone should i buy under 15k?</td>\n",
       "      <td>which is the best phone under 15000?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what will happen if superman punches his own f...</td>\n",
       "      <td>how strong can superman punch?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is the difference between front and back ...</td>\n",
       "      <td>what is the difference between front end and b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            clean_q1  \\\n",
       "0                how can i increase height after 22?   \n",
       "1          what do you do to get an adrenaline rush?   \n",
       "2                which phone should i buy under 15k?   \n",
       "3  what will happen if superman punches his own f...   \n",
       "4  what is the difference between front and back ...   \n",
       "\n",
       "                                            clean_q2  is_duplicate  \n",
       "0  what is the way to increase the height at the ...             1  \n",
       "1                        what is an adrenaline rush?             0  \n",
       "2               which is the best phone under 15000?             1  \n",
       "3                     how strong can superman punch?             0  \n",
       "4  what is the difference between front end and b...             1  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def second_lean(df):\n",
    "    def clean_str(input_str):\n",
    "        return input_str.replace(\"\\n\", \"\")\n",
    "    df[\"clean_q1\"] = df.apply(lambda x: clean_str(x[\"clean_q1\"]) ,1)\n",
    "    df[\"clean_q2\"] = df.apply(lambda x: clean_str(x[\"clean_q2\"]) ,1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_set = second_lean(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
